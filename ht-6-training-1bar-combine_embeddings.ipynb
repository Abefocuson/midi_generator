{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, IntEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from transformer_xl.default_txl import get_default_model\n",
    "from enum import Enum\n",
    "import torch\n",
    "from fastai.text.models.awd_lstm import *\n",
    "from fastai.text.models.transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=10, threshold=40, linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ht_encode import *\n",
    "import ht_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=8\n",
    "bptt=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai_data\n",
    "fastai_data.Y_OFFSET=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('data/midi/v7/midi_encode/np/hook_1bar_nopos/')\n",
    "# data = LMNPDataBunch.load(path, bs=bs, bptt=bptt, cache_name='tmp/sample')\n",
    "data = LMNPDataBunch.load(path, bs=bs, bptt=bptt, cache_name='tmp/all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tfmerXL_lm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX=enc_config.pad_idx+enc_config.enc_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 13, 5, 15, 15, 15, 15, 5, 7]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids_file = path/'tmp/all/train_ids.npy'\n",
    "all_ids = np.load(train_ids_file)\n",
    "id_cat = np.concatenate(all_ids); id_cat.shape\n",
    "ax = tuple(range(len(id_cat.shape)-1))\n",
    "max_vocab = id_cat.max(axis=ax)\n",
    "max_vocab = (max_vocab+1).tolist(); max_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BAR = 1\n",
    "N_COMPS = 9\n",
    "N_EMBS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_IDXS = [0,1,2,3,4,5]\n",
    "EMB_DIM = [N_EMBS]*len(EMB_IDXS)\n",
    "VOCAB_SZ = [max_vocab[i] for i in [iN,iNO,iND,iCI] + BIDX_ALL]\n",
    "EMB_MAP = list(zip(EMB_IDXS,VOCAB_SZ,EMB_DIM))\n",
    "eN,eO,eD,eI = EMB_MAP\n",
    "# eN,eO,eD,eI,eB,eM = EMB_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_WEIGHTS = [1,1,1,0.2,0.2,0.2,0.1,.2,.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2embidx = {\n",
    "    iN:eN,iNO:eO,iND:eD,\n",
    "    iC1:eN,iC2:eN,iC3:eN,iC4:eN,iCD:eD,iCI:eI,\n",
    "#     iB:eB,\n",
    "#     iM:eM\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 15, 128), (1, 13, 128), (2, 5, 128), (3, 7, 128)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMB_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaskType = Enum('MaskType', 'NoMask Sequential RandomWindow Bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['emb_map'] = EMB_MAP\n",
    "config['idx_map'] = idx2embidx\n",
    "config['mask_type'] = MaskType.RandomWindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_embs = sum([v[-1] for k,v in idx2embidx.items() if k not in BIDX_ALL])\n",
    "config['d_model'] = total_embs * N_BAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['ctx_len'] = 0\n",
    "config['mem_len'] = 512\n",
    "# config['d_inner'] = 1024 * N_BAR\n",
    "config['d_inner'] = config['d_model'] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BERT config\n",
    "# config['mask_type'] = MaskType.Bert\n",
    "# config['mem_len'] = 0\n",
    "# config['d_model'] = N_EMBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbed(nn.Module):\n",
    "    def __init__(self, emb_map, idx_map, embed_p:float=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        # note, octave, duration, instrument\n",
    "        self.idx_map = idx_map\n",
    "        self.emb_map = emb_map\n",
    "        embeddings = []\n",
    "        for idx,in_d,out_d in emb_map:\n",
    "            embeddings.append(nn.Embedding(in_d, out_d, padding_idx=PAD_IDX))\n",
    "        self.embeddings = nn.ModuleList(embeddings)\n",
    "        self.drop_emb = nn.Dropout(embed_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # batch x bptt x (n,o,d,i)\n",
    "        if BIDX_ALL: pos_enc = self.embeddings[eB[0]](x[...,iB]) + self.embeddings[eM[0]](x[...,iM])\n",
    "        embs = []\n",
    "        for i in range(x.shape[-1]):\n",
    "            emb_idx = self.idx_map[i][0]\n",
    "            if i in BIDX_ALL: continue\n",
    "            embx = self.embeddings[emb_idx](x[...,i])\n",
    "            if BIDX_ALL: embx.add_(pos_enc)\n",
    "            embs.append(embx)\n",
    "        emb = torch.stack(embs, dim=-2) # barlen x comp x emb\n",
    "#         emb = emb.permute(0,1,4,2,3) # for conv - emb x barlen x comp\n",
    "        return self.drop_emb(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TXLLinearDecoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, tie_encoder:nn.Module=None, bias:bool=True, input_dim=None):\n",
    "        super().__init__()\n",
    "        n_hid,n_out = tie_encoder.embedding_dim, tie_encoder.num_embeddings\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "            \n",
    "        if input_dim is not None and input_dim != n_hid:\n",
    "            self.decoder = nn.Sequential(nn.Linear(input_dim, n_hid), self.decoder)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.decoder(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDec(nn.Module):\n",
    "    def __init__(self, txl_emb, idx_map, output_p=0.0, out_bias=True, d_model=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        \n",
    "        decoders = []\n",
    "        for k,v in idx_map.items():\n",
    "            if k in BIDX_ALL: continue\n",
    "            emb = txl_emb.embeddings[v[0]]\n",
    "            decoder = TXLLinearDecoder(tie_encoder=emb, bias=out_bias, input_dim=d_model)\n",
    "            decoders.append(decoder)\n",
    "            \n",
    "        self.decoders = nn.ModuleList(decoders)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1])\n",
    "#         dim_shape = [N_BAR, N_COMPS, N_EMBS] if N_BAR > 1 else [N_COMPS, N_EMBS]\n",
    "#         output = output.view(*output.shape[:-1], *dim_shape)\n",
    "#         output = output.view(output.shape[0], -1, *dim_shape)\n",
    "        res = []\n",
    "        for idx,dec in enumerate(self.decoders):\n",
    "            res.append(dec(output))\n",
    "        return res, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_window_mask(x_len,m_len,device):\n",
    "    win_size,k = (np.random.randint(0,16)+1,0) if m_len > 0 else (1,1)\n",
    "    mem_mask = np.zeros((x_len,m_len))\n",
    "    tri_mask = np.triu(np.ones((x_len//win_size+1,x_len//win_size+1)),k=k)\n",
    "    window_mask = tri_mask.repeat(win_size,axis=0).repeat(win_size,axis=1)[:x_len,:x_len]\n",
    "    np_mask = np.concatenate((mem_mask, window_mask), axis=1)\n",
    "    mask = torch.tensor(np_mask, device=device).byte()[None,None]; mask\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_mask(x_len,m_len,device):\n",
    "    # create mask where only melody is hidden\n",
    "    # reshape input and remove 9 cats\n",
    "    rand = torch.rand(x_len,x_len)\n",
    "    mem_mask = torch.zeros(x_len,m_len).byte()\n",
    "    mask = rand < 2\n",
    "    comp = len(idx2embidx)\n",
    "    rand = torch.rand(x_len,x_len)\n",
    "    mem_mask = torch.zeros(x_len,m_len).byte()\n",
    "    mask = rand < 0.8\n",
    "#     mask = mask.view(bptt,comp,bptt,comp)\n",
    "#     mask[:,CIDX_ALL] = 0\n",
    "#     mask[...,CIDX_ALL] = 0\n",
    "#     mask = mask.view(x_len,x_len)\n",
    "    mask = torch.cat((mem_mask, mask), dim=-1)\n",
    "    return mask.byte().to(device)[None,None]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LMNPTransformerXL(nn.Module):\n",
    "    \"TransformerXL model: https://arxiv.org/abs/1901.02860.\"\n",
    "    def __init__(self, encoder, ctx_len:int, n_layers:int, n_heads:int, d_model:int, d_head:int, d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., embed_p:float=0., bias:bool=False, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadRelativeAttention,\n",
    "                 learned_pos_enc:bool=False, mask_type:MaskType=MaskType.Sequential, mem_len:int=0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.pos_enc = nn.Embedding(ctx_len, d_model) if learned_pos_enc else PositionalEncoding(d_model)\n",
    "        self.u = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.v = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.mem_len,self.n_layers,self.d_model,self.mask_type = mem_len,n_layers,d_model,mask_type\n",
    "        self.init = False\n",
    "        self.layers = nn.ModuleList([DecoderLayer(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                      ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "                      attn_cls=attn_cls) for k in range(n_layers)])\n",
    "    \n",
    "    def reset(self):\n",
    "        \"Reset the internal memory.\"\n",
    "        self.hidden = [next(self.parameters()).data.new(0) for i in range(self.n_layers+1)]\n",
    "\n",
    "    def _update_mems(self, hids):\n",
    "        if not getattr(self, 'hidden', False): return None\n",
    "        assert len(hids) == len(self.hidden), 'len(hids) != len(self.hidden)'\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(hids)):\n",
    "                cat = torch.cat([self.hidden[i], hids[i]], dim=1)\n",
    "                self.hidden[i] = cat[:,-self.mem_len:].detach()\n",
    "    \n",
    "    def select_hidden(self, idxs): self.hidden = [h[idxs] for h in self.hidden]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #The hidden state has to be initiliazed in the forward pass for nn.DataParallel\n",
    "        if self.mem_len > 0 and not self.init: \n",
    "            self.reset()\n",
    "            self.init = True\n",
    "        inp = self.encoder(x)\n",
    "        if self.mask_type.value == MaskType.Bert.value:\n",
    "            inp = inp.reshape(inp.shape[0], -1, inp.shape[-1]) # bs,bptt,comp,emb -> bs,bptt*comp,emb\n",
    "        else:\n",
    "            inp = inp.reshape(*inp.shape[:2], -1) # bs,bptt,comp,emb -> bs,bptt,comp,emb\n",
    "        bs,x_len = inp.shape[:2]\n",
    "        m_len = self.hidden[0].size(1) if hasattr(self, 'hidden') and len(self.hidden[0].size()) > 1 else 0\n",
    "        seq_len = m_len + x_len\n",
    "        \n",
    "        if self.mask_type.value == MaskType.NoMask.value: \n",
    "            self.mask = None\n",
    "        elif self.mask_type.value == MaskType.RandomWindow.value: \n",
    "            self.mask = rand_window_mask(x_len,m_len,x.device)\n",
    "        elif self.mask_type.value == MaskType.Sequential.value: \n",
    "            self.mask = torch.triu(x.new_ones(x_len, seq_len), diagonal=1+m_len).byte()[None,None]\n",
    "        elif self.mask_type.value == MaskType.Bert.value: \n",
    "            self.mask = bert_mask(x_len,m_len,x.device)\n",
    "        else: \n",
    "            raise ValueError('Unhandled mask type:', self.mask_type)\n",
    "        #[None,:,:None] for einsum implementation of attention\n",
    "        hids = []\n",
    "        pos = torch.arange(seq_len-1, -1, -1, device=inp.device, dtype=inp.dtype)\n",
    "        pos_enc = self.pos_enc(pos)\n",
    "        hids.append(inp)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            mem = self.hidden[i] if self.mem_len > 0 else None\n",
    "            inp = layer(inp, r=pos_enc, u=self.u, v=self.v, mask=self.mask, mem=mem)\n",
    "            hids.append(inp)\n",
    "        core_out = inp[:,-x_len:]\n",
    "        if self.mem_len > 0 : self._update_mems(hids)\n",
    "        return (self.hidden if self.mem_len > 0 else [core_out]),[core_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_model(config:dict=None, drop_mult:float=1.):\n",
    "    \"Create a language model from `arch` and its `config`, maybe `pretrained`.\"\n",
    "    for k in config.keys(): \n",
    "        if k.endswith('_p'): config[k] *= drop_mult\n",
    "    init = config.pop('init') if 'init' in config else None\n",
    "    \n",
    "    embed = TransformerEmbed(**config)\n",
    "    txl = LMNPTransformerXL(embed, **config)\n",
    "    decoder = TransformerDec(embed, **config)\n",
    "    model = SequentialRNN(txl, decoder)\n",
    "    \n",
    "    return model if init is None else model.apply(init)\n",
    "\n",
    "\n",
    "def language_model_learner(data:DataBunch, config:dict=None, drop_mult:float=1., pretrained:bool=True,\n",
    "                           **learn_kwargs) -> 'LanguageLearner':\n",
    "    \"Create a `Learner` with a language model from `data` and `arch`.\"\n",
    "    model = get_language_model(config=config, drop_mult=drop_mult)\n",
    "    learn = LanguageLearner(data, model, split_func=tfmer_lm_split, **learn_kwargs)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMNPLoss(nn.Module):\n",
    "    \"Same as `func`, but flattens input and target.\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX) \n",
    "        self.model = model\n",
    "        # not using func otherwise _loss_func_name2activ uses this attribute to get cross entropy loss\n",
    "\n",
    "    def __repr__(self): return f\"numpyenc loss of {self.fn}\"\n",
    "\n",
    "    def forward(self, inputs:Tensor, target:Tensor, **kwargs)->Rank0Tensor:\n",
    "        losses = []\n",
    "        \n",
    "#         target = target.clone()\n",
    "#         mask = self.model[0].mask\n",
    "#         ts_mask = mask[...,torch.eye(mask.shape[-1]).byte()]\n",
    "#         ts_mask = ts_mask.view(target.shape[1:])\n",
    "#         target[..., ~ts_mask] = PAD_IDX\n",
    "        \n",
    "        for idx,input in enumerate(inputs):\n",
    "#             if idx in CIDX_ALL: continue\n",
    "            t = target[...,idx]\n",
    "            input = input.view(-1,input.shape[-1])\n",
    "            losses.append(self.fn(input, t.view(-1))*LOSS_WEIGHTS[idx])\n",
    "        return sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmnp_accuracy(inputs:Tensor, target:Tensor, model)->Rank0Tensor:\n",
    "    \"Compute accuracy with `targs` when `input` is bs * n_classes.\"\n",
    "#     target = target.clone()\n",
    "#     mask = model[0].mask\n",
    "#     ts_mask = mask[...,torch.eye(mask.shape[-1]).byte()]\n",
    "#     ts_mask = ts_mask.view(target.shape[1:])\n",
    "#     target[..., ~ts_mask] = PAD_IDX\n",
    "    \n",
    "    inputs = [i.argmax(dim=-1).unsqueeze(dim=-1) for i in inputs]\n",
    "    input_cat = torch.cat(inputs, dim=-1)\n",
    "    target = target.view(input_cat.shape)\n",
    "    \n",
    "        \n",
    "    input_cat,target = input_cat.cpu().numpy(), target.cpu().numpy()\n",
    "    acc = (input_cat==target).astype(float)\n",
    "    acc[target==PAD_IDX] = np.nan\n",
    "#     acc[...,CIDX_ALL] = np.nan\n",
    "#     pdb.set_trace()\n",
    "    return torch.tensor(np.nanmean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data, config, clip=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = LMNPLoss(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_met = partial(lmnp_accuracy, model=learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics = [acc_met]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5672, device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob = data.one_batch(cpu=False)\n",
    "out = learn.pred_batch(ob)\n",
    "learn.loss_func([c.cuda() for c in out], ob[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # clip = 0.5\n",
    "# learn.lr_find(num_it=300)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.00% [1/5 14:53<59:33]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>lmnp_accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>1.614666</th>\n",
       "    <th>1.623789</th>\n",
       "    <th>0.913959</th>\n",
       "    <th>14:53</th>\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4289' class='' max='6277', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      68.33% [4289/6277 10:07<04:41 1.4632]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(5, 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('first_run_10ep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import basic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_func(parts): return [F.softmax(p, dim=-1) for p in parts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_name = camel2snake(learn.loss_func.__class__.__name__)\n",
    "basic_train.loss_func_name2activ[loss_func_name] = predict_func\n",
    "basic_train._loss_func2activ(learn.loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_idx = enc_config.enc_offset+enc_config.bos_idx\n",
    "\n",
    "def get_positions(xb):\n",
    "    beat_enc = xb[...,-1,:,iB]\n",
    "    enc_offset = enc_config.enc_offset\n",
    "    measure_enc = ((xb[...,-1,:,iM]-enc_offset+1)%8)+enc_offset\n",
    "    if (xb[:,-1]==enc_offset+enc_config.bos_idx).any(): \n",
    "#         print('Last timestep was BOS token. Currently unable to handle that. Returning')\n",
    "        raise Exception('Last timestep was BOS token. Currently unable to handle that')\n",
    "    return [beat_enc.squeeze(),measure_enc.squeeze()]\n",
    "\n",
    "def predict(self, xb, n_words:int=1, temperature:float=1., min_p:float=None):\n",
    "    \"Return the `n_words` that come after `text`.\"\n",
    "    ds = self.data.single_dl.dataset\n",
    "    self.model.reset()\n",
    "    learn.model[0].mask = False\n",
    "    if xb.shape[0] > 1: xb = xb[0][None]\n",
    "    seed = xb.cpu().numpy()\n",
    "    yb = torch.ones_like(xb)\n",
    "    timesteps = []\n",
    "    for _ in progress_bar(range(n_words), leave=True):\n",
    "        bar = get_positions(xb) if BIDX_ALL else []\n",
    "        outputs = learn.pred_batch(batch=(xb,yb))\n",
    "        for item in outputs: #progress_bar(range(n_words), leave=False):\n",
    "            res = item[0][-1]\n",
    "            if min_p is not None: \n",
    "                if (res >= min_p).float().sum() == 0:\n",
    "                    warn(f\"There is no item with probability >= {min_p}, try a lower value.\")\n",
    "                else: res[res < min_p] = 0.\n",
    "            if temperature != 1.: res.pow_(1 / temperature)\n",
    "            idx = torch.multinomial(res, 1)\n",
    "#             val,idx = torch.topk(res, 1)\n",
    "            bar.append(idx.squeeze().to(xb.device))\n",
    "        bar = torch.stack(bar, dim=-1)\n",
    "        if (bar==bos_idx).any(): \n",
    "            print('Predicted BOS token. Returning prediction...')\n",
    "            break\n",
    "        timesteps.append(bar.cpu().numpy())\n",
    "        xb = bar.clone().detach()[None,None] # don't use timesteps. use it's own memory instead\n",
    "    \n",
    "    learn.model[0].mask = True\n",
    "    return timesteps, seed.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = learn.data.one_batch(cpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_len = 80\n",
    "files = get_files(path/'hooktheory', extensions=['.npy'], recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading from specific file\n",
    "keywords = ['linkin', 'park']\n",
    "def contains_keywords(f): return all([k in str(f) for k in keywords])\n",
    "search = [f for f in files if contains_keywords(f)]; search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = np.random.choice(files)\n",
    "# file = search[9]\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_np = np.load(file)[:seed_len]\n",
    "xb = torch.tensor(seed_np).cuda()[None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, seed = predict(learn, xb, n_words=160, temperature=.5, min_p=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = dec_arr(np.array(out)); song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = song.to_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.show('midi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_song = dec_arr(seed)\n",
    "seed_stream = seed_song.to_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_stream.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_stream.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
