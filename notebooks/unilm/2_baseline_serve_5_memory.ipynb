{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.text import *\n",
    "from enum import Enum\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=10, threshold=40, linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "from src.fastai_data import *\n",
    "from src.encode_data import *\n",
    "from src.serve import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.music_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.unilm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = unilm_config(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['bs'] = 2\n",
    "# config['bptt'] = 1024\n",
    "# config['n_layers'] = 4\n",
    "# config['n_heads'] = 4\n",
    "# config['dec_layers'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['n_layers'] = 1\n",
    "config['n_layers'] = 0\n",
    "config['dec_layers'] = 8\n",
    "config['mem_len'] = 512\n",
    "config['rand_bptt'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctx_len': 150,\n",
       " 'n_layers': 0,\n",
       " 'n_heads': 8,\n",
       " 'd_model': 512,\n",
       " 'd_head': 64,\n",
       " 'd_inner': 2048,\n",
       " 'resid_p': 0.1,\n",
       " 'attn_p': 0.1,\n",
       " 'ff_p': 0.1,\n",
       " 'embed_p': 0.1,\n",
       " 'output_p': 0.1,\n",
       " 'bias': False,\n",
       " 'scale': True,\n",
       " 'act': <Activation.GeLU: 3>,\n",
       " 'double_drop': True,\n",
       " 'tie_weights': True,\n",
       " 'out_bias': True,\n",
       " 'init': <function fastai.text.models.transformer.init_transformer(m)>,\n",
       " 'mem_len': 512,\n",
       " 'mask': True,\n",
       " 'pad_idx': 1,\n",
       " 'bos_idx': 0,\n",
       " 'sep_idx': 8,\n",
       " 'transpose_range': (0, 12),\n",
       " 'rand_transpose': True,\n",
       " 'rand_bptt': False,\n",
       " 'note_range': (9, 139),\n",
       " 'bs': 16,\n",
       " 'bptt': 256,\n",
       " 'vocab_size': 274,\n",
       " 'dec_layers': 8}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path('../../data/midi/v15/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_name = 'tmp/hook_c'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ns_dl_tfms = [mask_tfm, next_sentence_tfm]\n",
    "# ns_config = config.copy()\n",
    "# ns_config['bs'] *= 2\n",
    "# ns_data = load_music_data(base_path/'piano_duet', cache_name=cache_name, vocab=vocab, \n",
    "#                           y_offset=0, dl_tfms=ns_dl_tfms, **ns_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2S Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2s_dl_tfms = [mask_s2s_tfm]\n",
    "# s2s_data = MusicDataBunch.load(base_path/'s2s_encode', cache_name=cache_name, \n",
    "#                            preloader_cls=S2SPreloader, dl_tfms=[mask_s2s_tfm], y_offset=1,\n",
    "#                            shuffle_dl=True, **config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NextWord dataset (Language Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_data = MusicDataBunch.load(base_path/'piano_duet', cache_name=cache_name, \n",
    "                              vocab=vocab, dl_tfms=[nw_tfm], y_offset=1, \n",
    "                              train_tfms=[to_single_stream], valid_tfms=[to_single_stream], **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = nw_data.one_batch(cpu=False); \n",
    "\n",
    "nw_data.train_dl.dl.dataset.update_rand_bptt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[  8, 143,  77,  ..., 141,   4,   4],\n",
       "          [  4, 143,  77,  ..., 141, 257,   4],\n",
       "          [  8, 143,  77,  ...,   4,   8,   4],\n",
       "          ...,\n",
       "          [  8,   4,  77,  ...,   4,   8,   4],\n",
       "          [  8, 143,  77,  ...,   4,   8,   4],\n",
       "          [  8, 143,   4,  ..., 141,   8,   4]], device='cuda:0'),\n",
       "  tensor([[4, 4, 4,  ..., 4, 4, 4],\n",
       "          [4, 4, 4,  ..., 4, 4, 4],\n",
       "          [4, 4, 4,  ..., 4, 4, 4],\n",
       "          ...,\n",
       "          [4, 4, 4,  ..., 4, 4, 4],\n",
       "          [4, 4, 4,  ..., 4, 4, 4],\n",
       "          [4, 4, 4,  ..., 4, 4, 4]], device='cuda:0'),\n",
       "  tensor([[  8, 143,  77,  ..., 141,   8, 141],\n",
       "          [  8, 143,  77,  ..., 141,   8, 141],\n",
       "          [  8, 143,  77,  ..., 141,   8, 141],\n",
       "          ...,\n",
       "          [  8, 143,  77,  ..., 141,   8, 141],\n",
       "          [  8, 143,  77,  ..., 141,   8, 141],\n",
       "          [  8, 143,  77,  ..., 141,   8, 141]], device='cuda:0')],\n",
       " [tensor([[  1,   1,   1,  ...,   1,   8, 141],\n",
       "          [  8,   1,   1,  ...,   1,   8, 141],\n",
       "          [  1,   1,   1,  ..., 141,   1, 141],\n",
       "          ...,\n",
       "          [  1, 143,   1,  ..., 141,   1, 141],\n",
       "          [  1,   1,   1,  ..., 141,   1, 141],\n",
       "          [  1,   1,  77,  ...,   1,   1, 141]], device='cuda:0'),\n",
       "  tensor([[143,  77, 151,  ...,   8, 141,  91],\n",
       "          [143,  77, 151,  ...,   8, 141,  91],\n",
       "          [143,  77, 151,  ...,   8, 141,  91],\n",
       "          ...,\n",
       "          [143,  77, 151,  ...,   8, 141,  91],\n",
       "          [143,  77, 151,  ...,   8, 141,  91],\n",
       "          [143,  77, 151,  ...,   8, 141,  91]], device='cuda:0')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([16, 256]),\n",
       " torch.Size([16, 256]),\n",
       " torch.Size([16, 256]),\n",
       " torch.Size([16, 256]),\n",
       " torch.Size([16, 256])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in xb+yb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = [ns_data, s2s_data, nw_data]\n",
    "datasets = [nw_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_func(parts): return [p if idx == 1 else F.softmax(p, dim=-1) for idx,p in enumerate(parts)]\n",
    "# Need to monkey patch pred_batch activation function for 2d array\n",
    "loss_func_name = camel2snake(BertLoss.__name__)\n",
    "basic_train.loss_func_name2activ[loss_func_name] = predict_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "loss_func = BertLoss(loss_mult=(0,1,1,1))\n",
    "learn = bert_model_learner(datasets[0], config.copy(), \n",
    "                           loss_func=loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics = [mask_acc, ns_acc, s2s_acc, nw_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks.rnn import RNNTrainer\n",
    "learn.callbacks = [c for c in learn.callbacks if not isinstance(c, RNNTrainer)]\n",
    "learn.callbacks.append(BertTrainer(learn, datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[src.unilm.BertTrainer]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(c) for c in learn.callbacks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(num_it=500)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.3719864, tensor(0.1024), tensor(0), tensor(0), tensor(0.)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mask_acc</th>\n",
       "      <th>ns_acc</th>\n",
       "      <th>s2s_acc</th>\n",
       "      <th>nw_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.673968</td>\n",
       "      <td>2.625121</td>\n",
       "      <td>0.068231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351949</td>\n",
       "      <td>03:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('sample_train_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sample train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../../data/midi/v15/piano_duet/models/sample_train_5.pth')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_path = base_path/'piano_duet/models/sample_train_5.pth'\n",
    "# state = torch.load(load_path, map_location='cpu')\n",
    "# get_model(learn.model).load_state_dict(state['model'])\n",
    "# load_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.load('sample_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../../data/midi/v15/models/unilm/all/1_ep24_best.pth'),\n",
       " PosixPath('../../data/midi/v15/models/unilm/all/2_ep80_best.pth'),\n",
       " PosixPath('../../data/midi/v15/models/unilm/all/1_ep24.pth')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_models = get_files(base_path/'models/unilm', recurse=True, extensions=['.pth']); saved_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BertHead:\n\tMissing key(s) in state_dict: \"s2s_decoder.layers.6.mha1.q_wgt.weight\", \"s2s_decoder.layers.6.mha1.k_wgt.weight\", \"s2s_decoder.layers.6.mha1.v_wgt.weight\", \"s2s_decoder.layers.6.mha1.out.weight\", \"s2s_decoder.layers.6.mha1.ln.weight\", \"s2s_decoder.layers.6.mha1.ln.bias\", \"s2s_decoder.layers.6.mha1.r_attn.weight\", \"s2s_decoder.layers.6.mha2.q_wgt.weight\", \"s2s_decoder.layers.6.mha2.k_wgt.weight\", \"s2s_decoder.layers.6.mha2.v_wgt.weight\", \"s2s_decoder.layers.6.mha2.out.weight\", \"s2s_decoder.layers.6.mha2.ln.weight\", \"s2s_decoder.layers.6.mha2.ln.bias\", \"s2s_decoder.layers.6.mha2.r_attn.weight\", \"s2s_decoder.layers.6.ff.layers.0.weight\", \"s2s_decoder.layers.6.ff.layers.0.bias\", \"s2s_decoder.layers.6.ff.layers.3.weight\", \"s2s_decoder.layers.6.ff.layers.3.bias\", \"s2s_decoder.layers.6.ff.layers.6.weight\", \"s2s_decoder.layers.6.ff.layers.6.bias\", \"s2s_decoder.layers.7.mha1.q_wgt.weight\", \"s2s_decoder.layers.7.mha1.k_wgt.weight\", \"s2s_decoder.layers.7.mha1.v_wgt.weight\", \"s2s_decoder.layers.7.mha1.out.weight\", \"s2s_decoder.layers.7.mha1.ln.weight\", \"s2s_decoder.layers.7.mha1.ln.bias\", \"s2s_decoder.layers.7.mha1.r_attn.weight\", \"s2s_decoder.layers.7.mha2.q_wgt.weight\", \"s2s_decoder.layers.7.mha2.k_wgt.weight\", \"s2s_decoder.layers.7.mha2.v_wgt.weight\", \"s2s_decoder.layers.7.mha2.out.weight\", \"s2s_decoder.layers.7.mha2.ln.weight\", \"s2s_decoder.layers.7.mha2.ln.bias\", \"s2s_decoder.layers.7.mha2.r_attn.weight\", \"s2s_decoder.layers.7.ff.layers.0.weight\", \"s2s_decoder.layers.7.ff.layers.0.bias\", \"s2s_decoder.layers.7.ff.layers.3.weight\", \"s2s_decoder.layers.7.ff.layers.3.bias\", \"s2s_decoder.layers.7.ff.layers.6.weight\", \"s2s_decoder.layers.7.ff.layers.6.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.layers.0.mhra.attention.weight\", \"encoder.layers.0.mhra.out.weight\", \"encoder.layers.0.mhra.ln.weight\", \"encoder.layers.0.mhra.ln.bias\", \"encoder.layers.0.mhra.r_attn.weight\", \"encoder.layers.0.ff.layers.0.weight\", \"encoder.layers.0.ff.layers.0.bias\", \"encoder.layers.0.ff.layers.3.weight\", \"encoder.layers.0.ff.layers.3.bias\", \"encoder.layers.0.ff.layers.6.weight\", \"encoder.layers.0.ff.layers.6.bias\", \"encoder.layers.1.mhra.attention.weight\", \"encoder.layers.1.mhra.out.weight\", \"encoder.layers.1.mhra.ln.weight\", \"encoder.layers.1.mhra.ln.bias\", \"encoder.layers.1.mhra.r_attn.weight\", \"encoder.layers.1.ff.layers.0.weight\", \"encoder.layers.1.ff.layers.0.bias\", \"encoder.layers.1.ff.layers.3.weight\", \"encoder.layers.1.ff.layers.3.bias\", \"encoder.layers.1.ff.layers.6.weight\", \"encoder.layers.1.ff.layers.6.bias\", \"encoder.layers.2.mhra.attention.weight\", \"encoder.layers.2.mhra.out.weight\", \"encoder.layers.2.mhra.ln.weight\", \"encoder.layers.2.mhra.ln.bias\", \"encoder.layers.2.mhra.r_attn.weight\", \"encoder.layers.2.ff.layers.0.weight\", \"encoder.layers.2.ff.layers.0.bias\", \"encoder.layers.2.ff.layers.3.weight\", \"encoder.layers.2.ff.layers.3.bias\", \"encoder.layers.2.ff.layers.6.weight\", \"encoder.layers.2.ff.layers.6.bias\", \"encoder.layers.3.mhra.attention.weight\", \"encoder.layers.3.mhra.out.weight\", \"encoder.layers.3.mhra.ln.weight\", \"encoder.layers.3.mhra.ln.bias\", \"encoder.layers.3.mhra.r_attn.weight\", \"encoder.layers.3.ff.layers.0.weight\", \"encoder.layers.3.ff.layers.0.bias\", \"encoder.layers.3.ff.layers.3.weight\", \"encoder.layers.3.ff.layers.3.bias\", \"encoder.layers.3.ff.layers.6.weight\", \"encoder.layers.3.ff.layers.6.bias\", \"encoder.layers.4.mhra.attention.weight\", \"encoder.layers.4.mhra.out.weight\", \"encoder.layers.4.mhra.ln.weight\", \"encoder.layers.4.mhra.ln.bias\", \"encoder.layers.4.mhra.r_attn.weight\", \"encoder.layers.4.ff.layers.0.weight\", \"encoder.layers.4.ff.layers.0.bias\", \"encoder.layers.4.ff.layers.3.weight\", \"encoder.layers.4.ff.layers.3.bias\", \"encoder.layers.4.ff.layers.6.weight\", \"encoder.layers.4.ff.layers.6.bias\", \"encoder.layers.5.mhra.attention.weight\", \"encoder.layers.5.mhra.out.weight\", \"encoder.layers.5.mhra.ln.weight\", \"encoder.layers.5.mhra.ln.bias\", \"encoder.layers.5.mhra.r_attn.weight\", \"encoder.layers.5.ff.layers.0.weight\", \"encoder.layers.5.ff.layers.0.bias\", \"encoder.layers.5.ff.layers.3.weight\", \"encoder.layers.5.ff.layers.3.bias\", \"encoder.layers.5.ff.layers.6.weight\", \"encoder.layers.5.ff.layers.6.bias\", \"encoder.layers.6.mhra.attention.weight\", \"encoder.layers.6.mhra.out.weight\", \"encoder.layers.6.mhra.ln.weight\", \"encoder.layers.6.mhra.ln.bias\", \"encoder.layers.6.mhra.r_attn.weight\", \"encoder.layers.6.ff.layers.0.weight\", \"encoder.layers.6.ff.layers.0.bias\", \"encoder.layers.6.ff.layers.3.weight\", \"encoder.layers.6.ff.layers.3.bias\", \"encoder.layers.6.ff.layers.6.weight\", \"encoder.layers.6.ff.layers.6.bias\", \"encoder.layers.7.mhra.attention.weight\", \"encoder.layers.7.mhra.out.weight\", \"encoder.layers.7.mhra.ln.weight\", \"encoder.layers.7.mhra.ln.bias\", \"encoder.layers.7.mhra.r_attn.weight\", \"encoder.layers.7.ff.layers.0.weight\", \"encoder.layers.7.ff.layers.0.bias\", \"encoder.layers.7.ff.layers.3.weight\", \"encoder.layers.7.ff.layers.3.bias\", \"encoder.layers.7.ff.layers.6.weight\", \"encoder.layers.7.ff.layers.6.bias\", \"encoder.layers.8.mhra.attention.weight\", \"encoder.layers.8.mhra.out.weight\", \"encoder.layers.8.mhra.ln.weight\", \"encoder.layers.8.mhra.ln.bias\", \"encoder.layers.8.mhra.r_attn.weight\", \"encoder.layers.8.ff.layers.0.weight\", \"encoder.layers.8.ff.layers.0.bias\", \"encoder.layers.8.ff.layers.3.weight\", \"encoder.layers.8.ff.layers.3.bias\", \"encoder.layers.8.ff.layers.6.weight\", \"encoder.layers.8.ff.layers.6.bias\", \"encoder.layers.9.mhra.attention.weight\", \"encoder.layers.9.mhra.out.weight\", \"encoder.layers.9.mhra.ln.weight\", \"encoder.layers.9.mhra.ln.bias\", \"encoder.layers.9.mhra.r_attn.weight\", \"encoder.layers.9.ff.layers.0.weight\", \"encoder.layers.9.ff.layers.0.bias\", \"encoder.layers.9.ff.layers.3.weight\", \"encoder.layers.9.ff.layers.3.bias\", \"encoder.layers.9.ff.layers.6.weight\", \"encoder.layers.9.ff.layers.6.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-a3de283019e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mload_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mload_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/midi/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 779\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    780\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertHead:\n\tMissing key(s) in state_dict: \"s2s_decoder.layers.6.mha1.q_wgt.weight\", \"s2s_decoder.layers.6.mha1.k_wgt.weight\", \"s2s_decoder.layers.6.mha1.v_wgt.weight\", \"s2s_decoder.layers.6.mha1.out.weight\", \"s2s_decoder.layers.6.mha1.ln.weight\", \"s2s_decoder.layers.6.mha1.ln.bias\", \"s2s_decoder.layers.6.mha1.r_attn.weight\", \"s2s_decoder.layers.6.mha2.q_wgt.weight\", \"s2s_decoder.layers.6.mha2.k_wgt.weight\", \"s2s_decoder.layers.6.mha2.v_wgt.weight\", \"s2s_decoder.layers.6.mha2.out.weight\", \"s2s_decoder.layers.6.mha2.ln.weight\", \"s2s_decoder.layers.6.mha2.ln.bias\", \"s2s_decoder.layers.6.mha2.r_attn.weight\", \"s2s_decoder.layers.6.ff.layers.0.weight\", \"s2s_decoder.layers.6.ff.layers.0.bias\", \"s2s_decoder.layers.6.ff.layers.3.weight\", \"s2s_decoder.layers.6.ff.layers.3.bias\", \"s2s_decoder.layers.6.ff.layers.6.weight\", \"s2s_decoder.layers.6.ff.layers.6.bias\", \"s2s_decoder.layers.7.mha1.q_wgt.weight\", \"s2s_decoder.layers.7.mha1.k_wgt.weight\", \"s2s_decoder.layers.7.mha1.v_wgt.weight\", \"s2s_decoder.layers.7.mha1.out.weight\", \"s2s_decoder.layers.7.mha1.ln.weight\", \"s2s_decoder.layers.7.mha1.ln.bias\", \"s2s_decoder.layers.7.mha1.r_attn.weight\", \"s2s_decoder.layers.7.mha2.q_wgt.weight\", \"s2s_decoder.layers.7.mha2.k_wgt.weight\", \"s2s_decoder.layers.7.mha2.v_wgt.weight\", \"s2s_decoder.layers.7.mha2.out.weight\", \"s2s_decoder.layers.7.mha2.ln.weight\", \"s2s_decoder.layers.7.mha2.ln.bias\", \"s2s_decoder.layers.7.mha2.r_attn.weight\", \"s2s_decoder.layers.7.ff.layers.0.weight\", \"s2s_decoder.layers.7.ff.layers.0.bias\", \"s2s_decoder.layers.7.ff.layers.3.weight\", \"s2s_decoder.layers.7.ff.layers.3.bias\", \"s2s_decoder.layers.7.ff.layers.6.weight\", \"s2s_decoder.layers.7.ff.layers.6.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.layers.0.mhra.attention.weight\", \"encoder.layers.0.mhra.out.weight\", \"encoder.layers.0.mhra.ln.weight\", \"encoder.layers.0.mhra.ln.bias\", \"encoder.layers.0.mhra.r_attn.weight\", \"encoder.layers.0.ff.layers.0.weight\", \"encoder.layers.0.ff.layers.0.bias\", \"encoder.layers.0.ff.layers.3.weight\", \"encoder.layers.0.ff.layers.3.bias\", \"encoder.layers.0.ff.layers.6.weight\", \"encoder.layers.0.ff.layers.6.bias\", \"encoder.layers.1.mhra.attention.weight\", \"encoder.layers.1.mhra.out.weight\", \"encoder.layers.1.mhra.ln.weight\", \"encoder.layers.1.mhra.ln.bias\", \"encoder.layers.1.mhra.r_attn.weight\", \"encoder.layers.1.ff.layers.0.weight\", \"encoder.layers.1.ff.layers.0.bias\", \"encoder.layers.1.ff.layers.3.weight\", \"encoder.layers.1.ff.layers.3.bias\", \"encoder.layers.1.ff.layers.6.weight\", \"encoder.layers.1.ff.layers.6.bias\", \"encoder.layers.2.mhra.attention.weight\", \"encoder.layers.2.mhra.out.weight\", \"encoder.layers.2.mhra.ln.weight\", \"encoder.layers.2.mhra.ln.bias\", \"encoder.layers.2.mhra.r_attn.weight\", \"encoder.layers.2.ff.layers.0.weight\", \"encoder.layers.2.ff.layers.0.bias\", \"encoder.layers.2.ff.layers.3.weight\", \"encoder.layers.2.ff.layers.3.bias\", \"encoder.layers.2.ff.layers.6.weight\", \"encoder.layers.2.ff.layers.6.bias\", \"encoder.layers.3.mhra.attention.weight\", \"encoder.layers.3.mhra.out.weight\", \"encoder.layers.3.mhra.ln.weight\", \"encoder.layers.3.mhra.ln.bias\", \"encoder.layers.3.mhra.r_attn.weight\", \"encoder.layers.3.ff.layers.0.weight\", \"encoder.layers.3.ff.layers.0.bias\", \"encoder.layers.3.ff.layers.3.weight\", \"encoder.layers.3.ff.layers.3.bias\", \"encoder.layers.3.ff.layers.6.weight\", \"encoder.layers.3.ff.layers.6.bias\", \"encoder.layers.4.mhra.attention.weight\", \"encoder.layers.4.mhra.out.weight\", \"encoder.layers.4.mhra.ln.weight\", \"encoder.layers.4.mhra.ln.bias\", \"encoder.layers.4.mhra.r_attn.weight\", \"encoder.layers.4.ff.layers.0.weight\", \"encoder.layers.4.ff.layers.0.bias\", \"encoder.layers.4.ff.layers.3.weight\", \"encoder.layers.4.ff.layers.3.bias\", \"encoder.layers.4.ff.layers.6.weight\", \"encoder.layers.4.ff.layers.6.bias\", \"encoder.layers.5.mhra.attention.weight\", \"encoder.layers.5.mhra.out.weight\", \"encoder.layers.5.mhra.ln.weight\", \"encoder.layers.5.mhra.ln.bias\", \"encoder.layers.5.mhra.r_attn.weight\", \"encoder.layers.5.ff.layers.0.weight\", \"encoder.layers.5.ff.layers.0.bias\", \"encoder.layers.5.ff.layers.3.weight\", \"encoder.layers.5.ff.layers.3.bias\", \"encoder.layers.5.ff.layers.6.weight\", \"encoder.layers.5.ff.layers.6.bias\", \"encoder.layers.6.mhra.attention.weight\", \"encoder.layers.6.mhra.out.weight\", \"encoder.layers.6.mhra.ln.weight\", \"encoder.layers.6.mhra.ln.bias\", \"encoder.layers.6.mhra.r_attn.weight\", \"encoder.layers.6.ff.layers.0.weight\", \"encoder.layers.6.ff.layers.0.bias\", \"encoder.layers.6.ff.layers.3.weight\", \"encoder.layers.6.ff.layers.3.bias\", \"encoder.layers.6.ff.layers.6.weight\", \"encoder.layers.6.ff.layers.6.bias\", \"encoder.layers.7.mhra.attention.weight\", \"encoder.layers.7.mhra.out.weight\", \"encoder.layers.7.mhra.ln.weight\", \"encoder.layers.7.mhra.ln.bias\", \"encoder.layers.7.mhra.r_attn.weight\", \"encoder.layers.7.ff.layers.0.weight\", \"encoder.layers.7.ff.layers.0.bias\", \"encoder.layers.7.ff.layers.3.weight\", \"encoder.layers.7.ff.layers.3.bias\", \"encoder.layers.7.ff.layers.6.weight\", \"encoder.layers.7.ff.layers.6.bias\", \"encoder.layers.8.mhra.attention.weight\", \"encoder.layers.8.mhra.out.weight\", \"encoder.layers.8.mhra.ln.weight\", \"encoder.layers.8.mhra.ln.bias\", \"encoder.layers.8.mhra.r_attn.weight\", \"encoder.layers.8.ff.layers.0.weight\", \"encoder.layers.8.ff.layers.0.bias\", \"encoder.layers.8.ff.layers.3.weight\", \"encoder.layers.8.ff.layers.3.bias\", \"encoder.layers.8.ff.layers.6.weight\", \"encoder.layers.8.ff.layers.6.bias\", \"encoder.layers.9.mhra.attention.weight\", \"encoder.layers.9.mhra.out.weight\", \"encoder.layers.9.mhra.ln.weight\", \"encoder.layers.9.mhra.ln.bias\", \"encoder.layers.9.mhra.r_attn.weight\", \"encoder.layers.9.ff.layers.0.weight\", \"encoder.layers.9.ff.layers.0.bias\", \"encoder.layers.9.ff.layers.3.weight\", \"encoder.layers.9.ff.layers.3.bias\", \"encoder.layers.9.ff.layers.6.weight\", \"encoder.layers.9.ff.layers.6.bias\". "
     ]
    }
   ],
   "source": [
    "# load_path = saved_models[1]\n",
    "# state = torch.load(load_path, map_location='cpu')\n",
    "# get_model(learn.model).load_state_dict(state['model'])\n",
    "# load_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/midi/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (0,1,7,10,12,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_url</th>\n",
       "      <th>section</th>\n",
       "      <th>numpy</th>\n",
       "      <th>ht_mode</th>\n",
       "      <th>midi</th>\n",
       "      <th>title</th>\n",
       "      <th>ht_time_signature</th>\n",
       "      <th>mxl</th>\n",
       "      <th>ht_offset</th>\n",
       "      <th>ht_bpm</th>\n",
       "      <th>ht_key</th>\n",
       "      <th>md5</th>\n",
       "      <th>midi_title</th>\n",
       "      <th>artist</th>\n",
       "      <th>genres</th>\n",
       "      <th>parts</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193179</th>\n",
       "      <td>https://www.hooktheory.com/theorytab/view/ritc...</td>\n",
       "      <td>chorus</td>\n",
       "      <td>piano_duet/hooktheory/pianoroll/r/ritchie-vale...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>midi_sources/hooktheory/pianoroll/r/ritchie-va...</td>\n",
       "      <td>la-bamba</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>C</td>\n",
       "      <td>aeee134e4034e5f98bb630c56d2f7f8c</td>\n",
       "      <td>La Bamba</td>\n",
       "      <td>ritchie-valen</td>\n",
       "      <td>Pop,Rock</td>\n",
       "      <td>chorus</td>\n",
       "      <td>hooktheory_c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 song_url section  \\\n",
       "193179  https://www.hooktheory.com/theorytab/view/ritc...  chorus   \n",
       "\n",
       "                                                    numpy  ht_mode  \\\n",
       "193179  piano_duet/hooktheory/pianoroll/r/ritchie-vale...      1.0   \n",
       "\n",
       "                                                     midi     title  \\\n",
       "193179  midi_sources/hooktheory/pianoroll/r/ritchie-va...  la-bamba   \n",
       "\n",
       "        ht_time_signature  mxl  ht_offset  ht_bpm ht_key  \\\n",
       "193179                4.0  NaN        0.0   144.0      C   \n",
       "\n",
       "                                     md5 midi_title         artist    genres  \\\n",
       "193179  aeee134e4034e5f98bb630c56d2f7f8c   La Bamba  ritchie-valen  Pop,Rock   \n",
       "\n",
       "         parts        source  \n",
       "193179  chorus  hooktheory_c  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = base_path/f'piano_duet/piano_duet.csv'\n",
    "csv = pd.read_csv(csv_path)\n",
    "df = csv.loc[csv['numpy'].notna()];\n",
    "\n",
    "# keywords = 'la bamba'.replace(' ', '|')\n",
    "title_filter = df['title'].str.contains('la-bamba')==True\n",
    "artist_filter = df['artist'].str.contains('')==True\n",
    "results = df[title_filter & artist_filter]; results.head()\n",
    "results = results[results.source == 'hooktheory_c']; results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('../../data/midi/v15/piano_duet/hooktheory/pianoroll/r/ritchie-valen/la-bamba/chorus_key_cmajor.npy'),\n",
       " PosixPath('../../data/midi/v15/midi_sources/hooktheory/pianoroll/r/ritchie-valen/la-bamba/chorus_key_cmajor.mid'))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "file = base_path/results.numpy.values[idx];\n",
    "orig = base_path/results.midi.values[idx];\n",
    "file, orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_len = 40\n",
    "song_np = np.load(file)\n",
    "seed_np = np.load(file)[:seed_len]\n",
    "xb = torch.tensor(to_single_stream(seed_np, vocab=vocab))[None]\n",
    "if torch.cuda.is_available(): xb = xb.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_stream = npenc2stream(seed_np)\n",
    "# seed_stream.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   8, 150,  64, 141,   8, 141,  66, 141,   8, 141,  68, 141,\n",
       "           8, 141,  69, 143,  64, 147,  61, 147,  57, 147,   8, 143,  73, 141,\n",
       "           8, 141,  76, 141,   8, 141,  74, 141,  69, 147,  66, 147,  62, 147,\n",
       "           8, 141,  74, 141,   8, 141,  78, 141,   8, 141,  76, 143,   8, 141,\n",
       "          59, 147,  56, 147,  52, 147,   8, 141,  64, 141,   8, 141,  68, 141,\n",
       "           8, 141,  71, 141,   8, 141,  74, 141,  69, 147,  66, 147]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = torch.full_like(xb, TaskType.NextWord.value)\n",
    "# res = self.pred_batch(batch=((xb,task_type,xb),(xb)))[-1][0, -1] # task1, task2 - (bs x ts x vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[0.0044, 0.0037, 0.0037,  ..., 0.0036, 0.0036, 0.0037],\n",
       "          [0.0034, 0.0047, 0.0037,  ..., 0.0036, 0.0037, 0.0036],\n",
       "          [0.0037, 0.0042, 0.0035,  ..., 0.0034, 0.0035, 0.0034],\n",
       "          ...,\n",
       "          [0.0031, 0.0036, 0.0033,  ..., 0.0033, 0.0033, 0.0033],\n",
       "          [0.0036, 0.0038, 0.0035,  ..., 0.0035, 0.0036, 0.0035],\n",
       "          [0.0031, 0.0036, 0.0033,  ..., 0.0033, 0.0033, 0.0033]]]),\n",
       " tensor([[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "          4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "          4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "          4, 4, 4, 4, 4, 4, 4, 4, 4, 4]]),\n",
       " tensor([[[9.1561e-01, 4.7759e-04, 1.6667e-04,  ..., 4.8221e-05,\n",
       "           4.0054e-05, 1.5997e-04],\n",
       "          [3.8568e-04, 1.8327e-01, 3.5651e-05,  ..., 1.3244e-05,\n",
       "           2.3935e-05, 2.4551e-05],\n",
       "          [2.4268e-06, 4.4008e-06, 3.0519e-08,  ..., 2.2241e-08,\n",
       "           5.8826e-08, 7.9160e-08],\n",
       "          ...,\n",
       "          [8.9230e-03, 6.2881e-04, 2.3953e-06,  ..., 1.9097e-06,\n",
       "           2.3953e-06, 2.3216e-06],\n",
       "          [5.4095e-06, 2.2244e-06, 5.1201e-08,  ..., 4.6802e-08,\n",
       "           8.8468e-08, 8.3760e-08],\n",
       "          [8.9300e-03, 6.2494e-04, 2.3899e-06,  ..., 1.9054e-06,\n",
       "           2.3899e-06, 2.2983e-06]]])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = learn.pred_batch(batch=((xb,task_type,xb),xb)); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[[ 0.1938,  0.0140,  0.0234,  ...,  0.0022, -0.0004,  0.0165],\n",
       "           [ 0.0140,  0.3479,  0.1149,  ...,  0.0867,  0.1130,  0.0864],\n",
       "           [-0.0170,  0.1159, -0.0717,  ..., -0.0933, -0.0694, -0.0841],\n",
       "           ...,\n",
       "           [-0.0279,  0.1076,  0.0144,  ...,  0.0207,  0.0187,  0.0314],\n",
       "           [-0.0104,  0.0472, -0.0249,  ..., -0.0374, -0.0192, -0.0329],\n",
       "           [-0.0279,  0.1076,  0.0144,  ...,  0.0207,  0.0187,  0.0314]]],\n",
       "         dtype=torch.float16),\n",
       "  tensor([[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "           4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "           4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "           4, 4, 4, 4, 4, 4, 4, 4, 4, 4]]),\n",
       "  tensor([[[ 6.0586, -2.8809, -3.6738,  ..., -4.3164, -4.9531, -4.1758],\n",
       "           [ 0.4409,  5.3867, -4.0352,  ..., -4.4609, -4.4492, -5.0898],\n",
       "           [-1.6709, -2.1230, -6.9414,  ..., -6.8086, -6.3633, -6.1953],\n",
       "           ...,\n",
       "           [-0.0349, -2.6992, -8.2500,  ..., -8.4688, -8.2656, -8.3047],\n",
       "           [-1.1816, -2.0820, -5.8516,  ..., -5.9258, -5.2969, -5.3594],\n",
       "           [-0.0339, -2.7012, -8.2500,  ..., -8.4688, -8.2578, -8.2969]]],\n",
       "         dtype=torch.float16)],\n",
       " tensor([[  1,   1,   1,  ...,   1,   8, 141],\n",
       "         [  8,   1,   1,  ...,   1,   8, 141],\n",
       "         [  1,   1,   1,  ..., 141,   1, 141],\n",
       "         ...,\n",
       "         [  1, 143,   1,  ..., 141,   1, 141],\n",
       "         [  1,   1,   1,  ..., 141,   1, 141],\n",
       "         [  1,   1,  77,  ...,   1,   1, 141]], device='cuda:0'))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = loss_batch(learn.model.eval(), (xb,task_type,xb), yb); preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0050)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last = res[-1][0, -1]; last[143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "idx = torch.multinomial(res[-1][0, -1], 1).item(); idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_nw(self, xb:Tensor, n_words:int=128):\n",
    "    if xb.shape[0] > 1: xb = xb[0][None]\n",
    "    seed = xb.cpu().numpy().squeeze()\n",
    "    yb = torch.ones_like(xb)\n",
    "    new_idx = []\n",
    "#     self.mask = False\n",
    "    self.model.reset()\n",
    "\n",
    "    for i in progress_bar(range(n_words), leave=True):\n",
    "        print(xb)\n",
    "        task_type = torch.full_like(xb, TaskType.NextWord.value)\n",
    "        \n",
    "        # Masking last - bert\n",
    "#         mask_last = torch.cat((xb, torch.tensor(vocab.mask_idx, device=xb.device).view(1, 1)), dim=-1)\n",
    "#         res = self.pred_batch(batch=((mask_last,task_type,mask_last),(yb)))[0][0, -1] # task1, task2 - (bs x ts x vocab)\n",
    "        \n",
    "        # Next Word\n",
    "        res = self.pred_batch(batch=((xb,task_type,xb),xb))[-1][0, -1] # task1, task2 - (bs x ts x vocab)\n",
    "\n",
    "        # Use first temperatures value if last prediction was duration\n",
    "        res.pow_(1 / (0.5))\n",
    "\n",
    "        idx = torch.multinomial(res, 1).item()\n",
    "    \n",
    "        new_idx.append(idx)\n",
    "#         t_idx = torch.tensor(idx, device=xb.device).view(1, 1)\n",
    "#         xb = torch.cat((xb, t_idx), dim=-1)\n",
    "        xb = xb.new_tensor([idx])[None]\n",
    "        \n",
    "#     self.mask = True\n",
    "    return np.array(new_idx), seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab.itos[143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0063)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if xb.shape[0] > 1: xb = xb[0][None]\n",
    "seed = xb.cpu().numpy().squeeze()\n",
    "yb = torch.ones_like(xb)\n",
    "res = learn.pred_batch(batch=((xb, torch.full_like(xb, TaskType.NextWord.value),xb),(yb)))\n",
    "res[-1][0, -1][141]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='40' class='' max='40', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [40/40 00:01<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   1,   8, 150,  64, 141,   8, 141,  66, 141,   8, 141,  68, 141,\n",
      "           8, 141,  69, 143,  64, 147,  61, 147,  57, 147,   8, 143,  73, 141,\n",
      "           8, 141,  76, 141,   8, 141,  74, 141,  69, 147,  66, 147,  62, 147,\n",
      "           8, 141,  74, 141,   8, 141,  78, 141,   8, 141,  76, 143,   8, 141,\n",
      "          59, 147,  56, 147,  52, 147,   8, 141,  64, 141,   8, 141,  68, 141,\n",
      "           8, 141,  71, 141,   8, 141,  74, 141,  69, 147,  66, 147]],\n",
      "       device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[143]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[143]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[8]], device='cuda:0')\n",
      "tensor([[141]], device='cuda:0')\n",
      "tensor([[73]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "out_s, seed_s = predict_nw(learn, xb, n_words=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd4',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd4',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'xxsep',\n",
       " 'd2',\n",
       " 'n64',\n",
       " 'd4']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab.itos[i] for i in out_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 49,
       "width": 250
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = npenc2stream(to_double_stream(out_s))\n",
    "stream.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
