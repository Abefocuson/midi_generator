{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.text import *\n",
    "from enum import Enum\n",
    "import torch\n",
    "from fastai.text.models.awd_lstm import *\n",
    "from fastai.text.models.transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=10, threshold=40, linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "from src.fastai_data import *\n",
    "from src.encode_data import *\n",
    "from src.serve import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lmnp_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../../data/midi/v15/piano_duet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Stream Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctx_len': 150,\n",
       " 'n_layers': 16,\n",
       " 'n_heads': 8,\n",
       " 'd_model': 256,\n",
       " 'd_head': 32,\n",
       " 'd_inner': 2048,\n",
       " 'resid_p': 0.1,\n",
       " 'attn_p': 0.1,\n",
       " 'ff_p': 0.1,\n",
       " 'embed_p': 0.1,\n",
       " 'output_p': 0.1,\n",
       " 'bias': False,\n",
       " 'scale': True,\n",
       " 'act': <Activation.GeLU: 3>,\n",
       " 'double_drop': True,\n",
       " 'tie_weights': True,\n",
       " 'out_bias': True,\n",
       " 'init': <function fastai.text.models.transformer.init_transformer(m)>,\n",
       " 'mem_len': 512,\n",
       " 'mask': True,\n",
       " 'pad_idx': 1,\n",
       " 'bos_idx': 0,\n",
       " 'sep_idx': 8,\n",
       " 'transpose_range': (0, 12),\n",
       " 'note_range': (9, 138),\n",
       " 'bs': 16,\n",
       " 'bptt': 256,\n",
       " 'vocab_size': 274}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = v15s_config(vocab); config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastai Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_tfms = [mask_tfm, next_sentence_tfm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLTFMS: [<function mask_tfm at 0x7f4ef013b8c8>, <function next_sentence_tfm at 0x7f4ef013b840>]\n"
     ]
    }
   ],
   "source": [
    "data = load_music_data(path, cache_name='tmp/sample', vocab=vocab, y_offset=0, dl_tfms=dl_tfms, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = data.one_batch(cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[155,  68, 155,  ...,   8, 147,  52],\n",
       "        [155,   4, 155,  ...,   8,   4,  52],\n",
       "        [155,   4, 155,  ...,   8, 147,   4],\n",
       "        ...,\n",
       "        [155,  65, 155,  ...,   8, 147,  46],\n",
       "        [ 53,  68, 155,  ...,   8, 147,   4],\n",
       "        [155,  68, 155,  ...,   8, 147,  52]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,  73,   1,  ...,   1, 147,   1],\n",
       "         [  1,  72,   1,  ...,   1,   1,  48],\n",
       "         ...,\n",
       "         [  1,  65,   1,  ...,   1,   1,   1],\n",
       "         [155,   1,   1,  ...,   1,   1,  49],\n",
       "         [  1,   1,   1,  ...,   1,   1,   1]], device='cuda:0'),\n",
       " tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_len = 0\n",
    "# x_len = 16 # bptt\n",
    "# seq_len = m_len+x_len\n",
    "# torch.triu(torch.ones(x_len, seq_len), diagonal=m_len).byte()[None,None].cpu().numpy()\n",
    "# torch.triu(torch.ones(x_len, seq_len), diagonal=m_len+1).byte()[None,None].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MusicTransformerXL(nn.Module):\n",
    "    \"Transformer model: https://arxiv.org/abs/1706.03762.\"\n",
    "    def __init__(self, vocab_sz:int, ctx_len:int, n_layers:int, n_heads:int, d_model:int, d_head:int, d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., embed_p:float=0., bias:bool=True, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadAttention,\n",
    "                 learned_pos_enc:bool=True, mask:bool=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.encoder = nn.Embedding(vocab_sz, d_model)\n",
    "        self.pos_enc = nn.Embedding(ctx_len, d_model) if learned_pos_enc else PositionalEncoding(d_model)\n",
    "        self.drop_emb = nn.Dropout(embed_p)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                      ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "                      attn_cls=attn_cls) for k in range(n_layers)])\n",
    "    \n",
    "    def reset(self): pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, x_len = x.size()\n",
    "        pos = torch.arange(0, x_len, device=x.device, dtype=x.dtype)\n",
    "        inp = self.drop_emb(self.encoder(x) + self.pos_enc(pos)[None]) #.mul_(self.d_model ** 0.5)\n",
    "        mask = torch.triu(x.new_ones(x_len, x_len), diagonal=1).byte()[None,None] if self.mask else None\n",
    "        #[None,:,:None] for einsum implementation of attention\n",
    "        for layer in self.layers: inp = layer(inp, mask=mask)\n",
    "        return ([inp],[inp]) #For the LinearDecoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.model[0].layers[0].mhra.residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.learner import _model_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['mem_len'] = 0\n",
    "config['mask'] = False\n",
    "config['ctx_len'] = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctx_len': 512,\n",
       " 'n_layers': 16,\n",
       " 'n_heads': 8,\n",
       " 'd_model': 256,\n",
       " 'd_head': 32,\n",
       " 'd_inner': 2048,\n",
       " 'resid_p': 0.1,\n",
       " 'attn_p': 0.1,\n",
       " 'ff_p': 0.1,\n",
       " 'embed_p': 0.1,\n",
       " 'output_p': 0.1,\n",
       " 'bias': False,\n",
       " 'scale': True,\n",
       " 'act': <Activation.GeLU: 3>,\n",
       " 'double_drop': True,\n",
       " 'tie_weights': True,\n",
       " 'out_bias': True,\n",
       " 'init': <function fastai.text.models.transformer.init_transformer(m)>,\n",
       " 'mem_len': 0,\n",
       " 'mask': False,\n",
       " 'pad_idx': 1,\n",
       " 'bos_idx': 0,\n",
       " 'sep_idx': 8,\n",
       " 'transpose_range': (0, 12),\n",
       " 'note_range': (9, 138),\n",
       " 'bs': 16,\n",
       " 'bptt': 256,\n",
       " 'vocab_size': 274}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_meta[MusicTransformerXL] = _model_meta[TransformerXL]\n",
    "_model_meta[MusicTransformerXL]['config_lm'] = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearDecoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_out:int, n_hid:int, output_p:float, tie_encoder:nn.Module=None, bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1])\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded, raw_outputs, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertHead(nn.Module):\n",
    "    def __init__(self, encoder, decoder, ns_decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.ns_decoder = ns_decoder\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_e = self.encoder(x)\n",
    "        return self.decoder(x_e), self.ns_decoder(x_e)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.encoder, self.decoder, self.ns_decoder][idx]\n",
    "        \n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in [self.encoder, self.decoder, self.ns_decoder]:\n",
    "            if hasattr(c, 'reset'): c.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NSDecoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_out:int, n_hid:int, bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs, outputs = input\n",
    "        decoded = self.decoder(outputs[-1])\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_music_model(arch:Callable, vocab_sz:int, config:dict=None, drop_mult:float=1.):\n",
    "    \"Create a language model from `arch` and its `config`, maybe `pretrained`.\"\n",
    "    meta = _model_meta[arch]\n",
    "    config = ifnone(config, meta['config_lm'].copy())\n",
    "    for k in config.keys(): \n",
    "        if k.endswith('_p'): config[k] *= drop_mult\n",
    "#     tie_weights,output_p,out_bias = map(config.pop, ['tie_weights', 'output_p', 'out_bias'])\n",
    "    tie_weights,output_p,out_bias = map(config.get, ['tie_weights', 'output_p', 'out_bias'])\n",
    "    init = config.pop('init') if 'init' in config else None\n",
    "    encoder = arch(vocab_sz, **config)\n",
    "    enc = encoder.encoder if tie_weights else None\n",
    "    decoder = LinearDecoder(vocab_sz, config[meta['hid_name']], output_p, tie_encoder=enc, bias=out_bias)\n",
    "    ns_decoder = NSDecoder(4, config[meta['hid_name']])\n",
    "    model = BertHead(encoder, decoder, ns_decoder)\n",
    "    return model if init is None else model.apply(init)\n",
    "\n",
    "\n",
    "def music_model_learner(arch:Callable, data:DataBunch, config:dict=None, drop_mult:float=1., pretrained:bool=False,\n",
    "                        pretrained_fnames:OptStrTuple=None, **learn_kwargs) -> 'LanguageLearner':\n",
    "    \"Create a `Learner` with a language model from `data` and `arch`.\"\n",
    "    model = get_music_model(arch, config['vocab_size'], config=config, drop_mult=drop_mult)\n",
    "    \n",
    "    meta = _model_meta[arch]\n",
    "    learn = MusicLearner(data, model, split_func=None, \n",
    "                         bos_idx=config['bos_idx'], sep_idx=config['sep_idx'],\n",
    "                        **learn_kwargs)\n",
    "    \n",
    "    if pretrained:\n",
    "        if 'url' not in meta: \n",
    "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "            return learn\n",
    "        model_path = untar_data(meta['url'], data=False)\n",
    "        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        learn.load_pretrained(*fnames)\n",
    "        learn.freeze()\n",
    "    if pretrained_fnames is not None:\n",
    "        fnames = [learn.path/learn.model_dir/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]\n",
    "        learn.load_pretrained(*fnames)\n",
    "        learn.freeze()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sep_idx: 8\n"
     ]
    }
   ],
   "source": [
    "learn = music_model_learner(MusicTransformerXL, data, config, drop_mult=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = data.one_batch(cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLoss():\n",
    "    def __init__(self, mask_loss, sent_loss):\n",
    "        self.mask_loss = mask_loss\n",
    "        self.sent_loss = sent_loss\n",
    "        \n",
    "    def __call__(self, input:Tensor, target:Tensor, target_sen:Tensor, **kwargs)->Rank0Tensor:\n",
    "        m = self.mask_loss.__call__(input[0], target, **kwargs)\n",
    "        s = self.sent_loss.__call__(input[1], target_sen, **kwargs)\n",
    "        return m + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks.rnn import RNNTrainer\n",
    "class BertRNNTrainer(RNNTrainer):\n",
    "    \"`Callback` that regroups lr adjustment to seq_len, AR and TAR.\"\n",
    "    def __init__(self, learn:Learner, alpha:float=0., beta:float=0.):\n",
    "        super().__init__(learn, alpha, beta)\n",
    "        \n",
    "#     def on_epoch_begin(self, **kwargs):\n",
    "#         \"Reset the hidden state of the model.\"\n",
    "#         self.learn.model.reset()\n",
    "\n",
    "    def on_loss_begin(self, last_output:Tuple[Tensor,Tensor,Tensor], **kwargs):\n",
    "        \"Save the extra outputs for later and only returns the true output.\"\n",
    "        return super().on_loss_begin(last_output[0])\n",
    "#         self.raw_out,self.out = last_output[1],last_output[2]\n",
    "#         return {'last_output': last_output[0]}\n",
    "\n",
    "#     def on_backward_begin(self, last_loss:Rank0Tensor, last_input:Tensor, **kwargs):\n",
    "#         \"Apply AR and TAR to `last_loss`.\"\n",
    "#         #AR and TAR\n",
    "#         if self.alpha != 0.:  last_loss += self.alpha * self.out[-1].float().pow(2).mean()\n",
    "#         if self.beta != 0.:\n",
    "#             h = self.raw_out[-1]\n",
    "#             if len(h)>1: last_loss += self.beta * (h[:,1:] - h[:,:-1]).float().pow(2).mean()\n",
    "#         return {'last_loss': last_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTrainer(LearnerCallback):\n",
    "    \"`Callback` that regroups lr adjustment to seq_len, AR and TAR.\"\n",
    "    def __init__(self, learn:Learner):\n",
    "        super().__init__(learn)\n",
    "        \n",
    "    def on_loss_begin(self, last_output:Tuple[Tensor,Tensor,Tensor], **kwargs):\n",
    "        \"Save the extra outputs for later and only returns the true output.\"\n",
    "        return {'last_output': (last_output[0][0], last_output[1][0]) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(learn.callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.callbacks = [BertTrainer(learn, alpha=2, beta=1)]\n",
    "learn.callbacks = [BertTrainer(learn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = BertLoss(CrossEntropyFlat(ignore_index=vocab.pad_idx), CrossEntropyFlat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[[-0.1301, -0.6223,  0.0786,  ..., -0.2835,  0.1422, -0.4134],\n",
       "           [-0.3381, -0.4479,  0.4134,  ..., -0.3283, -0.0706, -0.0867],\n",
       "           [ 0.0145,  0.4148,  0.1406,  ..., -0.0723, -0.5507, -0.3221],\n",
       "           ...,\n",
       "           [ 0.0192, -0.2977,  0.1053,  ..., -0.0690,  0.5979,  0.4539],\n",
       "           [ 0.3925, -0.0971, -0.2308,  ..., -0.2477, -0.3514, -0.2282],\n",
       "           [ 0.5407,  0.4429,  0.6588,  ..., -0.1949, -0.1373,  0.1382]],\n",
       "  \n",
       "          [[-0.1326, -0.6164,  0.0682,  ..., -0.2820,  0.1376, -0.4112],\n",
       "           [-0.3394, -0.4547,  0.4058,  ..., -0.3296, -0.0651, -0.0921],\n",
       "           [ 0.0141,  0.4151,  0.1405,  ..., -0.0723, -0.5508, -0.3222],\n",
       "           ...,\n",
       "           [ 0.0158, -0.2977,  0.0974,  ..., -0.0720,  0.5933,  0.4617],\n",
       "           [ 0.4061, -0.0913, -0.2396,  ..., -0.2529, -0.3579, -0.2237],\n",
       "           [ 0.5403,  0.4383,  0.6483,  ..., -0.1933, -0.1301,  0.1355]],\n",
       "  \n",
       "          [[-0.1270, -0.6277,  0.0812,  ..., -0.2772,  0.1389, -0.4079],\n",
       "           [-0.3388, -0.4551,  0.4056,  ..., -0.3299, -0.0644, -0.0920],\n",
       "           [ 0.0096,  0.4103,  0.1434,  ..., -0.0652, -0.5632, -0.3090],\n",
       "           ...,\n",
       "           [ 0.0195, -0.2978,  0.1051,  ..., -0.0692,  0.5985,  0.4540],\n",
       "           [ 0.3929, -0.0971, -0.2310,  ..., -0.2480, -0.3510, -0.2282],\n",
       "           [ 0.5410,  0.4427,  0.6587,  ..., -0.1952, -0.1367,  0.1383]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[-0.1309, -0.6170,  0.0695,  ..., -0.2824,  0.1528, -0.4087],\n",
       "           [-0.3396, -0.4550,  0.4055,  ..., -0.3297, -0.0650, -0.0921],\n",
       "           [ 0.0140,  0.4149,  0.1402,  ..., -0.0723, -0.5508, -0.3223],\n",
       "           ...,\n",
       "           [ 0.0188, -0.2977,  0.1050,  ..., -0.0690,  0.5979,  0.4538],\n",
       "           [ 0.3960, -0.0905, -0.2327,  ..., -0.2444, -0.3463, -0.2339],\n",
       "           [ 0.5401,  0.4381,  0.6481,  ..., -0.1934, -0.1301,  0.1355]],\n",
       "  \n",
       "          [[-0.1303, -0.6222,  0.0785,  ..., -0.2835,  0.1421, -0.4135],\n",
       "           [-0.3393, -0.4548,  0.4056,  ..., -0.3298, -0.0650, -0.0920],\n",
       "           [ 0.0091,  0.4106,  0.1434,  ..., -0.0651, -0.5640, -0.3092],\n",
       "           ...,\n",
       "           [ 0.0189, -0.2977,  0.1051,  ..., -0.0690,  0.5978,  0.4538],\n",
       "           [ 0.3952, -0.1008, -0.2364,  ..., -0.2580, -0.3412, -0.2292],\n",
       "           [ 0.5404,  0.4381,  0.6482,  ..., -0.1934, -0.1301,  0.1355]],\n",
       "  \n",
       "          [[-0.1313, -0.6283,  0.0802,  ..., -0.2864,  0.1320, -0.4032],\n",
       "           [-0.3380, -0.4478,  0.4131,  ..., -0.3281, -0.0703, -0.0868],\n",
       "           [ 0.0145,  0.4150,  0.1403,  ..., -0.0723, -0.5502, -0.3222],\n",
       "           ...,\n",
       "           [ 0.0192, -0.2976,  0.1051,  ..., -0.0690,  0.5983,  0.4538],\n",
       "           [ 0.4065, -0.0916, -0.2399,  ..., -0.2529, -0.3574, -0.2237],\n",
       "           [ 0.5398,  0.4531,  0.6568,  ..., -0.1967, -0.1305,  0.1330]]],\n",
       "         device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  [tensor([[[ 0.9900, -0.9356, -1.0522,  ..., -0.2779,  1.1581, -0.2776],\n",
       "            [ 0.6684,  0.7689, -1.6655,  ..., -1.8633, -0.8776, -0.1641],\n",
       "            [ 0.3834,  0.3885,  0.4587,  ...,  0.8918, -1.6120,  1.1861],\n",
       "            ...,\n",
       "            [ 0.1877,  0.0664,  1.0406,  ...,  0.4612,  0.7369,  2.0480],\n",
       "            [ 0.5019,  0.8204,  0.7142,  ...,  0.8510,  1.6422, -0.2844],\n",
       "            [ 0.0824, -0.6897,  1.2731,  ...,  0.0477,  0.4769,  0.2527]],\n",
       "   \n",
       "           [[ 0.9544, -0.9563, -1.0739,  ..., -0.2891,  1.1236, -0.3074],\n",
       "            [ 0.7024,  0.7512, -1.6659,  ..., -1.8771, -0.8981, -0.1515],\n",
       "            [ 0.3839,  0.3875,  0.4596,  ...,  0.8921, -1.6124,  1.1841],\n",
       "            ...,\n",
       "            [ 0.1871,  0.0826,  1.0707,  ...,  0.4989,  0.7792,  2.0372],\n",
       "            [ 0.5181,  0.8111,  0.7027,  ...,  0.8548,  1.6335, -0.2978],\n",
       "            [ 0.1040, -0.7085,  1.2797,  ...,  0.0636,  0.4725,  0.2663]],\n",
       "   \n",
       "           [[ 0.9686, -0.9149, -1.1069,  ..., -0.2849,  1.1470, -0.3040],\n",
       "            [ 0.7028,  0.7511, -1.6666,  ..., -1.8766, -0.8977, -0.1479],\n",
       "            [ 0.3819,  0.4050,  0.4889,  ...,  0.9256, -1.5998,  1.1437],\n",
       "            ...,\n",
       "            [ 0.1885,  0.0657,  1.0408,  ...,  0.4622,  0.7366,  2.0496],\n",
       "            [ 0.5030,  0.8199,  0.7144,  ...,  0.8520,  1.6420, -0.2828],\n",
       "            [ 0.0832, -0.6904,  1.2729,  ...,  0.0487,  0.4767,  0.2545]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[ 0.9794, -0.8999, -1.0698,  ..., -0.2771,  1.1520, -0.2504],\n",
       "            [ 0.7023,  0.7524, -1.6658,  ..., -1.8782, -0.8972, -0.1509],\n",
       "            [ 0.3840,  0.3889,  0.4595,  ...,  0.8911, -1.6113,  1.1844],\n",
       "            ...,\n",
       "            [ 0.1881,  0.0667,  1.0413,  ...,  0.4609,  0.7377,  2.0468],\n",
       "            [ 0.5270,  0.7840,  0.6924,  ...,  0.7780,  1.6302, -0.2683],\n",
       "            [ 0.1039, -0.7072,  1.2796,  ...,  0.0626,  0.4732,  0.2669]],\n",
       "   \n",
       "           [[ 0.9898, -0.9360, -1.0510,  ..., -0.2771,  1.1578, -0.2790],\n",
       "            [ 0.7013,  0.7514, -1.6656,  ..., -1.8769, -0.8981, -0.1510],\n",
       "            [ 0.3806,  0.4053,  0.4898,  ...,  0.9252, -1.5999,  1.1406],\n",
       "            ...,\n",
       "            [ 0.1874,  0.0660,  1.0417,  ...,  0.4620,  0.7366,  2.0467],\n",
       "            [ 0.5414,  0.7898,  0.7108,  ...,  0.8078,  1.6188, -0.2744],\n",
       "            [ 0.1031, -0.7080,  1.2798,  ...,  0.0638,  0.4723,  0.2669]],\n",
       "   \n",
       "           [[ 0.9797, -0.9238, -1.0819,  ..., -0.2563,  1.1121, -0.2821],\n",
       "            [ 0.6692,  0.7684, -1.6648,  ..., -1.8622, -0.8778, -0.1632],\n",
       "            [ 0.3843,  0.3878,  0.4591,  ...,  0.8923, -1.6124,  1.1867],\n",
       "            ...,\n",
       "            [ 0.1884,  0.0657,  1.0411,  ...,  0.4620,  0.7365,  2.0487],\n",
       "            [ 0.5183,  0.8113,  0.7021,  ...,  0.8550,  1.6335, -0.2953],\n",
       "            [ 0.0650, -0.7265,  1.2846,  ...,  0.0475,  0.4863,  0.2605]]],\n",
       "          device='cuda:0', grad_fn=<AddcmulBackward>)],\n",
       "  [tensor([[[ 0.9900, -0.9356, -1.0522,  ..., -0.2779,  1.1581, -0.2776],\n",
       "            [ 0.6684,  0.7689, -1.6655,  ..., -1.8633, -0.8776, -0.1641],\n",
       "            [ 0.3834,  0.3885,  0.4587,  ...,  0.8918, -1.6120,  1.1861],\n",
       "            ...,\n",
       "            [ 0.1877,  0.0664,  1.0406,  ...,  0.4612,  0.7369,  2.0480],\n",
       "            [ 0.5019,  0.8204,  0.7142,  ...,  0.8510,  1.6422, -0.2844],\n",
       "            [ 0.0824, -0.6897,  1.2731,  ...,  0.0477,  0.4769,  0.2527]],\n",
       "   \n",
       "           [[ 0.9544, -0.9563, -1.0739,  ..., -0.2891,  1.1236, -0.3074],\n",
       "            [ 0.7024,  0.7512, -1.6659,  ..., -1.8771, -0.8981, -0.1515],\n",
       "            [ 0.3839,  0.3875,  0.4596,  ...,  0.8921, -1.6124,  1.1841],\n",
       "            ...,\n",
       "            [ 0.1871,  0.0826,  1.0707,  ...,  0.4989,  0.7792,  2.0372],\n",
       "            [ 0.5181,  0.8111,  0.7027,  ...,  0.8548,  1.6335, -0.2978],\n",
       "            [ 0.1040, -0.7085,  1.2797,  ...,  0.0636,  0.4725,  0.2663]],\n",
       "   \n",
       "           [[ 0.9686, -0.9149, -1.1069,  ..., -0.2849,  1.1470, -0.3040],\n",
       "            [ 0.7028,  0.7511, -1.6666,  ..., -1.8766, -0.8977, -0.1479],\n",
       "            [ 0.3819,  0.4050,  0.4889,  ...,  0.9256, -1.5998,  1.1437],\n",
       "            ...,\n",
       "            [ 0.1885,  0.0657,  1.0408,  ...,  0.4622,  0.7366,  2.0496],\n",
       "            [ 0.5030,  0.8199,  0.7144,  ...,  0.8520,  1.6420, -0.2828],\n",
       "            [ 0.0832, -0.6904,  1.2729,  ...,  0.0487,  0.4767,  0.2545]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[ 0.9794, -0.8999, -1.0698,  ..., -0.2771,  1.1520, -0.2504],\n",
       "            [ 0.7023,  0.7524, -1.6658,  ..., -1.8782, -0.8972, -0.1509],\n",
       "            [ 0.3840,  0.3889,  0.4595,  ...,  0.8911, -1.6113,  1.1844],\n",
       "            ...,\n",
       "            [ 0.1881,  0.0667,  1.0413,  ...,  0.4609,  0.7377,  2.0468],\n",
       "            [ 0.5270,  0.7840,  0.6924,  ...,  0.7780,  1.6302, -0.2683],\n",
       "            [ 0.1039, -0.7072,  1.2796,  ...,  0.0626,  0.4732,  0.2669]],\n",
       "   \n",
       "           [[ 0.9898, -0.9360, -1.0510,  ..., -0.2771,  1.1578, -0.2790],\n",
       "            [ 0.7013,  0.7514, -1.6656,  ..., -1.8769, -0.8981, -0.1510],\n",
       "            [ 0.3806,  0.4053,  0.4898,  ...,  0.9252, -1.5999,  1.1406],\n",
       "            ...,\n",
       "            [ 0.1874,  0.0660,  1.0417,  ...,  0.4620,  0.7366,  2.0467],\n",
       "            [ 0.5414,  0.7898,  0.7108,  ...,  0.8078,  1.6188, -0.2744],\n",
       "            [ 0.1031, -0.7080,  1.2798,  ...,  0.0638,  0.4723,  0.2669]],\n",
       "   \n",
       "           [[ 0.9797, -0.9238, -1.0819,  ..., -0.2563,  1.1121, -0.2821],\n",
       "            [ 0.6692,  0.7684, -1.6648,  ..., -1.8622, -0.8778, -0.1632],\n",
       "            [ 0.3843,  0.3878,  0.4591,  ...,  0.8923, -1.6124,  1.1867],\n",
       "            ...,\n",
       "            [ 0.1884,  0.0657,  1.0411,  ...,  0.4620,  0.7365,  2.0487],\n",
       "            [ 0.5183,  0.8113,  0.7021,  ...,  0.8550,  1.6335, -0.2953],\n",
       "            [ 0.0650, -0.7265,  1.2846,  ...,  0.0475,  0.4863,  0.2605]]],\n",
       "          device='cuda:0', grad_fn=<AddcmulBackward>)]),\n",
       " (tensor([[[ 0.1871,  0.0514,  0.0239,  0.6817],\n",
       "           [ 0.4680,  0.6333,  0.3433,  0.2949],\n",
       "           [ 0.0443,  0.0491, -0.1483, -0.4911],\n",
       "           ...,\n",
       "           [-0.2761, -0.2051, -0.0602, -0.1949],\n",
       "           [-0.7194,  0.1641,  0.1838,  0.1393],\n",
       "           [-0.9185,  0.4534, -0.3390, -0.1472]],\n",
       "  \n",
       "          [[ 0.1859,  0.0326,  0.0254,  0.6888],\n",
       "           [ 0.4706,  0.6362,  0.3509,  0.2980],\n",
       "           [ 0.0445,  0.0487, -0.1483, -0.4913],\n",
       "           ...,\n",
       "           [-0.2695, -0.2034, -0.0608, -0.1974],\n",
       "           [-0.7208,  0.1648,  0.1796,  0.1413],\n",
       "           [-0.9196,  0.4480, -0.3336, -0.1432]],\n",
       "  \n",
       "          [[ 0.1816,  0.0443,  0.0263,  0.6864],\n",
       "           [ 0.4709,  0.6367,  0.3513,  0.2977],\n",
       "           [ 0.0551,  0.0506, -0.1446, -0.4935],\n",
       "           ...,\n",
       "           [-0.2757, -0.2049, -0.0597, -0.1952],\n",
       "           [-0.7190,  0.1644,  0.1844,  0.1390],\n",
       "           [-0.9179,  0.4535, -0.3385, -0.1475]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 0.1787,  0.0358,  0.0303,  0.6893],\n",
       "           [ 0.4707,  0.6362,  0.3508,  0.2979],\n",
       "           [ 0.0447,  0.0488, -0.1483, -0.4913],\n",
       "           ...,\n",
       "           [-0.2758, -0.2055, -0.0602, -0.1951],\n",
       "           [-0.7220,  0.1583,  0.1860,  0.1386],\n",
       "           [-0.9194,  0.4481, -0.3337, -0.1431]],\n",
       "  \n",
       "          [[ 0.1871,  0.0513,  0.0238,  0.6817],\n",
       "           [ 0.4704,  0.6364,  0.3508,  0.2982],\n",
       "           [ 0.0546,  0.0501, -0.1450, -0.4932],\n",
       "           ...,\n",
       "           [-0.2760, -0.2052, -0.0602, -0.1948],\n",
       "           [-0.7165,  0.1761,  0.1823,  0.1333],\n",
       "           [-0.9197,  0.4483, -0.3336, -0.1429]],\n",
       "  \n",
       "          [[ 0.2007,  0.0328,  0.0337,  0.6923],\n",
       "           [ 0.4686,  0.6332,  0.3436,  0.2945],\n",
       "           [ 0.0450,  0.0490, -0.1480, -0.4915],\n",
       "           ...,\n",
       "           [-0.2756, -0.2053, -0.0598, -0.1953],\n",
       "           [-0.7204,  0.1651,  0.1800,  0.1410],\n",
       "           [-0.9234,  0.4451, -0.3416, -0.1465]]], device='cuda:0',\n",
       "         grad_fn=<AddBackward0>),\n",
       "  [tensor([[[ 0.9900, -0.9356, -1.0522,  ..., -0.2779,  1.1581, -0.2776],\n",
       "            [ 0.6684,  0.7689, -1.6655,  ..., -1.8633, -0.8776, -0.1641],\n",
       "            [ 0.3834,  0.3885,  0.4587,  ...,  0.8918, -1.6120,  1.1861],\n",
       "            ...,\n",
       "            [ 0.1877,  0.0664,  1.0406,  ...,  0.4612,  0.7369,  2.0480],\n",
       "            [ 0.5019,  0.8204,  0.7142,  ...,  0.8510,  1.6422, -0.2844],\n",
       "            [ 0.0824, -0.6897,  1.2731,  ...,  0.0477,  0.4769,  0.2527]],\n",
       "   \n",
       "           [[ 0.9544, -0.9563, -1.0739,  ..., -0.2891,  1.1236, -0.3074],\n",
       "            [ 0.7024,  0.7512, -1.6659,  ..., -1.8771, -0.8981, -0.1515],\n",
       "            [ 0.3839,  0.3875,  0.4596,  ...,  0.8921, -1.6124,  1.1841],\n",
       "            ...,\n",
       "            [ 0.1871,  0.0826,  1.0707,  ...,  0.4989,  0.7792,  2.0372],\n",
       "            [ 0.5181,  0.8111,  0.7027,  ...,  0.8548,  1.6335, -0.2978],\n",
       "            [ 0.1040, -0.7085,  1.2797,  ...,  0.0636,  0.4725,  0.2663]],\n",
       "   \n",
       "           [[ 0.9686, -0.9149, -1.1069,  ..., -0.2849,  1.1470, -0.3040],\n",
       "            [ 0.7028,  0.7511, -1.6666,  ..., -1.8766, -0.8977, -0.1479],\n",
       "            [ 0.3819,  0.4050,  0.4889,  ...,  0.9256, -1.5998,  1.1437],\n",
       "            ...,\n",
       "            [ 0.1885,  0.0657,  1.0408,  ...,  0.4622,  0.7366,  2.0496],\n",
       "            [ 0.5030,  0.8199,  0.7144,  ...,  0.8520,  1.6420, -0.2828],\n",
       "            [ 0.0832, -0.6904,  1.2729,  ...,  0.0487,  0.4767,  0.2545]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[ 0.9794, -0.8999, -1.0698,  ..., -0.2771,  1.1520, -0.2504],\n",
       "            [ 0.7023,  0.7524, -1.6658,  ..., -1.8782, -0.8972, -0.1509],\n",
       "            [ 0.3840,  0.3889,  0.4595,  ...,  0.8911, -1.6113,  1.1844],\n",
       "            ...,\n",
       "            [ 0.1881,  0.0667,  1.0413,  ...,  0.4609,  0.7377,  2.0468],\n",
       "            [ 0.5270,  0.7840,  0.6924,  ...,  0.7780,  1.6302, -0.2683],\n",
       "            [ 0.1039, -0.7072,  1.2796,  ...,  0.0626,  0.4732,  0.2669]],\n",
       "   \n",
       "           [[ 0.9898, -0.9360, -1.0510,  ..., -0.2771,  1.1578, -0.2790],\n",
       "            [ 0.7013,  0.7514, -1.6656,  ..., -1.8769, -0.8981, -0.1510],\n",
       "            [ 0.3806,  0.4053,  0.4898,  ...,  0.9252, -1.5999,  1.1406],\n",
       "            ...,\n",
       "            [ 0.1874,  0.0660,  1.0417,  ...,  0.4620,  0.7366,  2.0467],\n",
       "            [ 0.5414,  0.7898,  0.7108,  ...,  0.8078,  1.6188, -0.2744],\n",
       "            [ 0.1031, -0.7080,  1.2798,  ...,  0.0638,  0.4723,  0.2669]],\n",
       "   \n",
       "           [[ 0.9797, -0.9238, -1.0819,  ..., -0.2563,  1.1121, -0.2821],\n",
       "            [ 0.6692,  0.7684, -1.6648,  ..., -1.8622, -0.8778, -0.1632],\n",
       "            [ 0.3843,  0.3878,  0.4591,  ...,  0.8923, -1.6124,  1.1867],\n",
       "            ...,\n",
       "            [ 0.1884,  0.0657,  1.0411,  ...,  0.4620,  0.7365,  2.0487],\n",
       "            [ 0.5183,  0.8113,  0.7021,  ...,  0.8550,  1.6335, -0.2953],\n",
       "            [ 0.0650, -0.7265,  1.2846,  ...,  0.0475,  0.4863,  0.2605]]],\n",
       "          device='cuda:0', grad_fn=<AddcmulBackward>)],\n",
       "  [tensor([[[ 0.9900, -0.9356, -1.0522,  ..., -0.2779,  1.1581, -0.2776],\n",
       "            [ 0.6684,  0.7689, -1.6655,  ..., -1.8633, -0.8776, -0.1641],\n",
       "            [ 0.3834,  0.3885,  0.4587,  ...,  0.8918, -1.6120,  1.1861],\n",
       "            ...,\n",
       "            [ 0.1877,  0.0664,  1.0406,  ...,  0.4612,  0.7369,  2.0480],\n",
       "            [ 0.5019,  0.8204,  0.7142,  ...,  0.8510,  1.6422, -0.2844],\n",
       "            [ 0.0824, -0.6897,  1.2731,  ...,  0.0477,  0.4769,  0.2527]],\n",
       "   \n",
       "           [[ 0.9544, -0.9563, -1.0739,  ..., -0.2891,  1.1236, -0.3074],\n",
       "            [ 0.7024,  0.7512, -1.6659,  ..., -1.8771, -0.8981, -0.1515],\n",
       "            [ 0.3839,  0.3875,  0.4596,  ...,  0.8921, -1.6124,  1.1841],\n",
       "            ...,\n",
       "            [ 0.1871,  0.0826,  1.0707,  ...,  0.4989,  0.7792,  2.0372],\n",
       "            [ 0.5181,  0.8111,  0.7027,  ...,  0.8548,  1.6335, -0.2978],\n",
       "            [ 0.1040, -0.7085,  1.2797,  ...,  0.0636,  0.4725,  0.2663]],\n",
       "   \n",
       "           [[ 0.9686, -0.9149, -1.1069,  ..., -0.2849,  1.1470, -0.3040],\n",
       "            [ 0.7028,  0.7511, -1.6666,  ..., -1.8766, -0.8977, -0.1479],\n",
       "            [ 0.3819,  0.4050,  0.4889,  ...,  0.9256, -1.5998,  1.1437],\n",
       "            ...,\n",
       "            [ 0.1885,  0.0657,  1.0408,  ...,  0.4622,  0.7366,  2.0496],\n",
       "            [ 0.5030,  0.8199,  0.7144,  ...,  0.8520,  1.6420, -0.2828],\n",
       "            [ 0.0832, -0.6904,  1.2729,  ...,  0.0487,  0.4767,  0.2545]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[ 0.9794, -0.8999, -1.0698,  ..., -0.2771,  1.1520, -0.2504],\n",
       "            [ 0.7023,  0.7524, -1.6658,  ..., -1.8782, -0.8972, -0.1509],\n",
       "            [ 0.3840,  0.3889,  0.4595,  ...,  0.8911, -1.6113,  1.1844],\n",
       "            ...,\n",
       "            [ 0.1881,  0.0667,  1.0413,  ...,  0.4609,  0.7377,  2.0468],\n",
       "            [ 0.5270,  0.7840,  0.6924,  ...,  0.7780,  1.6302, -0.2683],\n",
       "            [ 0.1039, -0.7072,  1.2796,  ...,  0.0626,  0.4732,  0.2669]],\n",
       "   \n",
       "           [[ 0.9898, -0.9360, -1.0510,  ..., -0.2771,  1.1578, -0.2790],\n",
       "            [ 0.7013,  0.7514, -1.6656,  ..., -1.8769, -0.8981, -0.1510],\n",
       "            [ 0.3806,  0.4053,  0.4898,  ...,  0.9252, -1.5999,  1.1406],\n",
       "            ...,\n",
       "            [ 0.1874,  0.0660,  1.0417,  ...,  0.4620,  0.7366,  2.0467],\n",
       "            [ 0.5414,  0.7898,  0.7108,  ...,  0.8078,  1.6188, -0.2744],\n",
       "            [ 0.1031, -0.7080,  1.2798,  ...,  0.0638,  0.4723,  0.2669]],\n",
       "   \n",
       "           [[ 0.9797, -0.9238, -1.0819,  ..., -0.2563,  1.1121, -0.2821],\n",
       "            [ 0.6692,  0.7684, -1.6648,  ..., -1.8622, -0.8778, -0.1632],\n",
       "            [ 0.3843,  0.3878,  0.4591,  ...,  0.8923, -1.6124,  1.1867],\n",
       "            ...,\n",
       "            [ 0.1884,  0.0657,  1.0411,  ...,  0.4620,  0.7365,  2.0487],\n",
       "            [ 0.5183,  0.8113,  0.7021,  ...,  0.8550,  1.6335, -0.2953],\n",
       "            [ 0.0650, -0.7265,  1.2846,  ...,  0.0475,  0.4863,  0.2605]]],\n",
       "          device='cuda:0', grad_fn=<AddcmulBackward>)]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_acc(input:Tensor, t1:Tensor, t2:Tensor)->Rank0Tensor:\n",
    "    n = t1.shape[0]\n",
    "    input = input[0].argmax(dim=-1).view(n,-1)\n",
    "    t1 = t1.view(n,-1)\n",
    "    mask = t1 != vocab.pad_idx\n",
    "    return (input[mask]==t1[mask]).float().mean()\n",
    "\n",
    "def ns_acc(input:Tensor, t1:Tensor, t2:Tensor)->Rank0Tensor:\n",
    "    return accuracy(input[1], t2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics = [mask_acc, ns_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.1036386, tensor(0.0038), tensor(0.2199)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mask_acc</th>\n",
       "      <th>ns_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.230405</td>\n",
       "      <td>4.143900</td>\n",
       "      <td>0.250067</td>\n",
       "      <td>0.369497</td>\n",
       "      <td>01:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.166582</td>\n",
       "      <td>4.094497</td>\n",
       "      <td>0.188318</td>\n",
       "      <td>0.324444</td>\n",
       "      <td>01:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.105020</td>\n",
       "      <td>4.131458</td>\n",
       "      <td>0.248072</td>\n",
       "      <td>0.417856</td>\n",
       "      <td>01:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(3, 1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
