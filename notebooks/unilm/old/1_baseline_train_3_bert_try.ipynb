{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.text import *\n",
    "from enum import Enum\n",
    "import torch\n",
    "from fastai.text.models.awd_lstm import *\n",
    "from fastai.text.models.transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=10, threshold=40, linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "from src.fastai_data import *\n",
    "from src.encode_data import *\n",
    "from src.serve import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lmnp_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../../data/midi/v15/piano_duet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Stream Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctx_len': 150,\n",
       " 'n_layers': 16,\n",
       " 'n_heads': 8,\n",
       " 'd_model': 256,\n",
       " 'd_head': 32,\n",
       " 'd_inner': 2048,\n",
       " 'resid_p': 0.1,\n",
       " 'attn_p': 0.1,\n",
       " 'ff_p': 0.1,\n",
       " 'embed_p': 0.1,\n",
       " 'output_p': 0.1,\n",
       " 'bias': False,\n",
       " 'scale': True,\n",
       " 'act': <Activation.GeLU: 3>,\n",
       " 'double_drop': True,\n",
       " 'tie_weights': True,\n",
       " 'out_bias': True,\n",
       " 'init': <function fastai.text.models.transformer.init_transformer(m)>,\n",
       " 'mem_len': 512,\n",
       " 'mask': True,\n",
       " 'pad_idx': 1,\n",
       " 'bos_idx': 0,\n",
       " 'sep_idx': 8,\n",
       " 'transpose_range': (0, 12),\n",
       " 'note_range': (9, 138),\n",
       " 'bs': 16,\n",
       " 'bptt': 256,\n",
       " 'vocab_size': 274}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = v15s_config(vocab); config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastai Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_tfms = [mask_tfm, next_sentence_tfm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLTFMS: [<function mask_tfm at 0x7fc03c0d67b8>, <function next_sentence_tfm at 0x7fc03c0d6730>]\n"
     ]
    }
   ],
   "source": [
    "data = load_music_data(path, cache_name='tmp/sample', vocab=vocab, y_offset=0, dl_tfms=dl_tfms, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = data.one_batch(cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[155,  70, 155,  ...,   8, 147,  52],\n",
       "        [155,   4, 130,  ...,   4, 147,  47],\n",
       "        [155,  63, 155,  ...,   4, 147,  52],\n",
       "        ...,\n",
       "        [  4,  68,   4,  ...,   8, 147,  52],\n",
       "        [155,  68, 155,  ...,   8, 147,  52],\n",
       "        [  4,   4, 155,  ...,   8, 147,  52]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,  69, 155,  ...,   8,   1,   1],\n",
       "         [  1,   1,   1,  ...,   8,   1,   1],\n",
       "         ...,\n",
       "         [155,   1, 155,  ...,   1,   1,   1],\n",
       "         [  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [155,  69, 155,  ...,   1,   1,   1]], device='cuda:0'),\n",
       " tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_len = 0\n",
    "# x_len = 16 # bptt\n",
    "# seq_len = m_len+x_len\n",
    "# torch.triu(torch.ones(x_len, seq_len), diagonal=m_len).byte()[None,None].cpu().numpy()\n",
    "# torch.triu(torch.ones(x_len, seq_len), diagonal=m_len+1).byte()[None,None].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicAttention(MultiHeadRelativeAttention):\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, resid_p:float=0., attn_p:float=0., bias:bool=True,\n",
    "                 scale:bool=True, residual:bool=True):\n",
    "        super().__init__(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.residual = residual\n",
    "        \n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs):\n",
    "        if self.residual: return self.ln(x + self.drop_res(self.out(self._apply_attention(x, mask=mask, **kwargs))))\n",
    "        return self.ln(self.drop_res(self.out(self._apply_attention(x, mask=mask, **kwargs))))\n",
    "    \n",
    "class MusicTransformerXL(nn.Module):\n",
    "    \"TransformerXL model: https://arxiv.org/abs/1901.02860.\"\n",
    "    def __init__(self, vocab_sz:int, ctx_len:int, n_layers:int, n_heads:int, d_model:int, d_head:int, d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., embed_p:float=0., bias:bool=False, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MusicAttention,\n",
    "                 learned_pos_enc:bool=False, mask:bool=True, mem_len:int=0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Embedding(vocab_sz, d_model)\n",
    "        self.pos_enc = nn.Embedding(ctx_len, d_model) if learned_pos_enc else PositionalEncoding(d_model)\n",
    "        self.drop_emb = nn.Dropout(embed_p)\n",
    "        self.u = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.v = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.mem_len,self.n_layers,self.d_model,self.mask = mem_len,n_layers,d_model,mask\n",
    "        self.init = False\n",
    "        self.layers = nn.ModuleList([DecoderLayer(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                      ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "                      attn_cls=attn_cls) for k in range(n_layers)])\n",
    "#         self.layers[0].mhra.residual = False\n",
    "    \n",
    "    def reset(self):\n",
    "        \"Reset the internal memory.\"\n",
    "        self.hidden = [next(self.parameters()).data.new(0) for i in range(self.n_layers+1)]\n",
    "\n",
    "    def _update_mems(self, hids):\n",
    "        if not getattr(self, 'hidden', False): return None\n",
    "        assert len(hids) == len(self.hidden), 'len(hids) != len(self.hidden)'\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(hids)):\n",
    "                cat = torch.cat([self.hidden[i], hids[i]], dim=1)\n",
    "                self.hidden[i] = cat[:,-self.mem_len:].detach()\n",
    "    \n",
    "    def select_hidden(self, idxs): self.hidden = [h[idxs] for h in self.hidden]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #The hidden state has to be initiliazed in the forward pass for nn.DataParallel\n",
    "        if self.mem_len > 0 and not self.init: \n",
    "            self.reset()\n",
    "            self.init = True\n",
    "        bs,x_len = x.size()\n",
    "        inp = self.drop_emb(self.encoder(x)) #.mul_(self.d_model ** 0.5)\n",
    "        m_len = self.hidden[0].size(1) if hasattr(self, 'hidden') and len(self.hidden[0].size()) > 1 else 0\n",
    "        seq_len = m_len + x_len\n",
    "#         mask = torch.triu(x.new_ones(x_len, seq_len), diagonal=m_len+1).byte()[None,None] if self.mask else None\n",
    "        mask = torch.triu(x.new_ones(x_len, seq_len), diagonal=m_len).byte()[None,None] if self.mask else None\n",
    "        if m_len == 0 and self.mask: mask[0,0] = 0\n",
    "        #[None,:,:None] for einsum implementation of attention\n",
    "        hids = []\n",
    "        pos = torch.arange(seq_len-1, -1, -1, device=inp.device, dtype=inp.dtype)\n",
    "        pos_enc = self.pos_enc(pos)\n",
    "        hids.append(inp)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            mem = self.hidden[i] if self.mem_len > 0 else None\n",
    "            inp = layer(inp, r=pos_enc, u=self.u, v=self.v, mask=mask, mem=mem)\n",
    "            hids.append(inp)\n",
    "        core_out = inp[:,-x_len:]\n",
    "        if self.mem_len > 0 : self._update_mems(hids)\n",
    "        return (self.hidden if self.mem_len > 0 else [core_out]),[core_out]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.model[0].layers[0].mhra.residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.learner import _model_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['mem_len'] = 0\n",
    "config['mask'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctx_len': 150,\n",
       " 'n_layers': 16,\n",
       " 'n_heads': 8,\n",
       " 'd_model': 256,\n",
       " 'd_head': 32,\n",
       " 'd_inner': 2048,\n",
       " 'resid_p': 0.1,\n",
       " 'attn_p': 0.1,\n",
       " 'ff_p': 0.1,\n",
       " 'embed_p': 0.1,\n",
       " 'output_p': 0.1,\n",
       " 'bias': False,\n",
       " 'scale': True,\n",
       " 'act': <Activation.GeLU: 3>,\n",
       " 'double_drop': True,\n",
       " 'tie_weights': True,\n",
       " 'out_bias': True,\n",
       " 'init': <function fastai.text.models.transformer.init_transformer(m)>,\n",
       " 'mem_len': 0,\n",
       " 'mask': False,\n",
       " 'pad_idx': 1,\n",
       " 'bos_idx': 0,\n",
       " 'sep_idx': 8,\n",
       " 'transpose_range': (0, 12),\n",
       " 'note_range': (9, 138),\n",
       " 'bs': 16,\n",
       " 'bptt': 256,\n",
       " 'vocab_size': 274}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_meta[MusicTransformerXL] = _model_meta[TransformerXL]\n",
    "_model_meta[MusicTransformerXL]['config_lm'] = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearDecoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_out:int, n_hid:int, output_p:float, tie_encoder:nn.Module=None, bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1])\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded, raw_outputs, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertHead(nn.Module):\n",
    "    def __init__(self, encoder, decoder, ns_decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.ns_decoder = ns_decoder\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_e = self.encoder(x)\n",
    "        return self.decoder(x_e), self.ns_decoder(x_e)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.encoder, self.decoder, self.ns_decoder][idx]\n",
    "        \n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in [self.encoder, self.decoder, self.ns_decoder]:\n",
    "            if hasattr(c, 'reset'): c.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NSDecoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_out:int, n_hid:int, bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs, outputs = input\n",
    "        decoded = self.decoder(outputs[-1])\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_music_model(arch:Callable, vocab_sz:int, config:dict=None, drop_mult:float=1.):\n",
    "    \"Create a language model from `arch` and its `config`, maybe `pretrained`.\"\n",
    "    meta = _model_meta[arch]\n",
    "    config = ifnone(config, meta['config_lm'].copy())\n",
    "    for k in config.keys(): \n",
    "        if k.endswith('_p'): config[k] *= drop_mult\n",
    "#     tie_weights,output_p,out_bias = map(config.pop, ['tie_weights', 'output_p', 'out_bias'])\n",
    "    tie_weights,output_p,out_bias = map(config.get, ['tie_weights', 'output_p', 'out_bias'])\n",
    "    init = config.pop('init') if 'init' in config else None\n",
    "    encoder = arch(vocab_sz, **config)\n",
    "    enc = encoder.encoder if tie_weights else None\n",
    "    decoder = LinearDecoder(vocab_sz, config[meta['hid_name']], output_p, tie_encoder=enc, bias=out_bias)\n",
    "    ns_decoder = NSDecoder(4, config[meta['hid_name']])\n",
    "    model = BertHead(encoder, decoder, ns_decoder)\n",
    "    return model if init is None else model.apply(init)\n",
    "\n",
    "\n",
    "def music_model_learner(arch:Callable, data:DataBunch, config:dict=None, drop_mult:float=1., pretrained:bool=False,\n",
    "                        pretrained_fnames:OptStrTuple=None, **learn_kwargs) -> 'LanguageLearner':\n",
    "    \"Create a `Learner` with a language model from `data` and `arch`.\"\n",
    "    model = get_music_model(arch, config['vocab_size'], config=config, drop_mult=drop_mult)\n",
    "    \n",
    "    meta = _model_meta[arch]\n",
    "    learn = MusicLearner(data, model, split_func=meta['split_lm'], \n",
    "                         bos_idx=config['bos_idx'], sep_idx=config['sep_idx'],\n",
    "                        **learn_kwargs)\n",
    "    \n",
    "    if pretrained:\n",
    "        if 'url' not in meta: \n",
    "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "            return learn\n",
    "        model_path = untar_data(meta['url'], data=False)\n",
    "        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        learn.load_pretrained(*fnames)\n",
    "        learn.freeze()\n",
    "    if pretrained_fnames is not None:\n",
    "        fnames = [learn.path/learn.model_dir/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]\n",
    "        learn.load_pretrained(*fnames)\n",
    "        learn.freeze()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sep_idx: 8\n"
     ]
    }
   ],
   "source": [
    "learn = music_model_learner(MusicTransformerXL, data, config, drop_mult=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = data.one_batch(cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLoss():\n",
    "    def __init__(self, mask_loss, sent_loss):\n",
    "        self.mask_loss = mask_loss\n",
    "        self.sent_loss = sent_loss\n",
    "        \n",
    "    def __call__(self, input:Tensor, target:Tensor, target_sen:Tensor, **kwargs)->Rank0Tensor:\n",
    "        m = self.mask_loss.__call__(input[0], target, **kwargs)\n",
    "        s = self.sent_loss.__call__(input[1], target_sen, **kwargs)\n",
    "        return m + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks.rnn import RNNTrainer\n",
    "class BertRNNTrainer(RNNTrainer):\n",
    "    \"`Callback` that regroups lr adjustment to seq_len, AR and TAR.\"\n",
    "    def __init__(self, learn:Learner, alpha:float=0., beta:float=0.):\n",
    "        super().__init__(learn, alpha, beta)\n",
    "        \n",
    "#     def on_epoch_begin(self, **kwargs):\n",
    "#         \"Reset the hidden state of the model.\"\n",
    "#         self.learn.model.reset()\n",
    "\n",
    "    def on_loss_begin(self, last_output:Tuple[Tensor,Tensor,Tensor], **kwargs):\n",
    "        \"Save the extra outputs for later and only returns the true output.\"\n",
    "        return super().on_loss_begin(last_output[0])\n",
    "#         self.raw_out,self.out = last_output[1],last_output[2]\n",
    "#         return {'last_output': last_output[0]}\n",
    "\n",
    "#     def on_backward_begin(self, last_loss:Rank0Tensor, last_input:Tensor, **kwargs):\n",
    "#         \"Apply AR and TAR to `last_loss`.\"\n",
    "#         #AR and TAR\n",
    "#         if self.alpha != 0.:  last_loss += self.alpha * self.out[-1].float().pow(2).mean()\n",
    "#         if self.beta != 0.:\n",
    "#             h = self.raw_out[-1]\n",
    "#             if len(h)>1: last_loss += self.beta * (h[:,1:] - h[:,:-1]).float().pow(2).mean()\n",
    "#         return {'last_loss': last_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTrainer(LearnerCallback):\n",
    "    \"`Callback` that regroups lr adjustment to seq_len, AR and TAR.\"\n",
    "    def __init__(self, learn:Learner):\n",
    "        super().__init__(learn)\n",
    "        \n",
    "    def on_loss_begin(self, last_output:Tuple[Tensor,Tensor,Tensor], **kwargs):\n",
    "        \"Save the extra outputs for later and only returns the true output.\"\n",
    "        return {'last_output': (last_output[0][0], last_output[1][0]) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(learn.callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.callbacks = [BertTrainer(learn, alpha=2, beta=1)]\n",
    "learn.callbacks = [BertTrainer(learn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = BertLoss(CrossEntropyFlat(ignore_index=vocab.pad_idx), CrossEntropyFlat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[[-0.3912, -0.6980, -0.3038,  ..., -0.2446, -0.5346, -0.0691],\n",
       "           [ 0.1125, -0.0110, -0.0520,  ...,  0.0340, -0.2118,  0.0357],\n",
       "           [ 0.1985,  0.5913,  0.2297,  ...,  0.2983, -0.2505, -0.4206],\n",
       "           ...,\n",
       "           [ 0.5575,  0.3192, -0.6309,  ...,  0.4049, -0.0618,  0.0721],\n",
       "           [-0.2353,  0.1912, -0.5863,  ..., -0.4431,  0.0875, -0.1718],\n",
       "           [ 0.1123, -0.0115, -0.0516,  ...,  0.0334, -0.2120,  0.0364]],\n",
       "  \n",
       "          [[ 0.2424, -0.3601,  0.2058,  ..., -0.2098, -0.0800,  0.1691],\n",
       "           [ 0.1039, -0.0134, -0.0544,  ...,  0.0349, -0.2073,  0.0507],\n",
       "           [ 0.1879,  0.5892,  0.2277,  ...,  0.2984, -0.2418, -0.4045],\n",
       "           ...,\n",
       "           [ 0.5503,  0.3191, -0.6325,  ...,  0.4059, -0.0538,  0.0854],\n",
       "           [-0.2446,  0.1901, -0.5880,  ..., -0.4411,  0.0977, -0.1598],\n",
       "           [ 0.1045, -0.0136, -0.0539,  ...,  0.0344, -0.2073,  0.0509]],\n",
       "  \n",
       "          [[-0.3943, -0.7061, -0.3050,  ..., -0.2442, -0.5487, -0.0752],\n",
       "           [ 0.1044, -0.0203, -0.0528,  ...,  0.0349, -0.2275,  0.0242],\n",
       "           [ 0.1918,  0.5928,  0.2264,  ...,  0.2999, -0.2619, -0.4327],\n",
       "           ...,\n",
       "           [ 0.5557,  0.3121, -0.6345,  ...,  0.4022, -0.0731,  0.0633],\n",
       "           [ 0.1261, -0.4747,  0.1202,  ...,  0.2971, -0.1979,  0.0220],\n",
       "           [ 0.1053, -0.0210, -0.0528,  ...,  0.0349, -0.2277,  0.0249]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[-0.0955, -0.0783,  0.1138,  ..., -0.0719,  0.0767, -0.0747],\n",
       "           [ 0.0989, -0.0278, -0.0468,  ...,  0.0376, -0.2176,  0.0182],\n",
       "           [ 0.1863,  0.5823,  0.2344,  ...,  0.3013, -0.2525, -0.4337],\n",
       "           ...,\n",
       "           [ 0.5482,  0.3078, -0.6233,  ...,  0.4043, -0.0602,  0.0634],\n",
       "           [ 0.2373, -0.3716,  0.2132,  ..., -0.2141, -0.0901,  0.1410],\n",
       "           [ 0.0991, -0.0281, -0.0462,  ...,  0.0368, -0.2183,  0.0187]],\n",
       "  \n",
       "          [[ 0.4228, -0.2488, -0.3260,  ...,  0.4540,  0.3594, -0.4408],\n",
       "           [ 0.1020, -0.0229, -0.0638,  ...,  0.0360, -0.2257,  0.0336],\n",
       "           [ 0.1880,  0.5885,  0.2164,  ...,  0.3007, -0.2590, -0.4201],\n",
       "           ...,\n",
       "           [ 0.5512,  0.3110, -0.6398,  ...,  0.4037, -0.0679,  0.0718],\n",
       "           [ 0.2434, -0.3644,  0.1962,  ..., -0.2121, -0.0959,  0.1551],\n",
       "           [ 0.1012, -0.0228, -0.0634,  ...,  0.0361, -0.2259,  0.0343]],\n",
       "  \n",
       "          [[-0.3825, -0.6974, -0.2995,  ..., -0.2429, -0.5486, -0.0735],\n",
       "           [ 0.1176, -0.0139, -0.0437,  ...,  0.0370, -0.2258,  0.0261],\n",
       "           [-0.7549, -0.0029, -0.0979,  ...,  0.0170, -0.4936, -0.1108],\n",
       "           ...,\n",
       "           [ 0.5697,  0.3193, -0.6252,  ...,  0.4047, -0.0738,  0.0703],\n",
       "           [ 0.4650,  0.2182, -0.2789,  ..., -0.2763,  0.6108, -0.5116],\n",
       "           [ 0.1181, -0.0147, -0.0430,  ...,  0.0366, -0.2257,  0.0267]]],\n",
       "         device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  [tensor([[[ 1.2025,  0.4747,  0.6522,  ...,  1.4155,  0.4779, -0.2181],\n",
       "            [-0.6222,  1.0280, -0.0088,  ..., -0.4985,  0.3699, -0.0513],\n",
       "            [-0.3761,  1.1681,  0.4032,  ..., -0.0977,  1.2847, -0.4356],\n",
       "            ...,\n",
       "            [-0.4127,  1.5183, -0.3478,  ..., -0.7941,  0.5797, -0.2687],\n",
       "            [ 0.9969,  1.0722, -0.3944,  ...,  0.5261,  0.0170,  0.4011],\n",
       "            [-0.6215,  1.0269, -0.0113,  ..., -0.4951,  0.3709, -0.0535]],\n",
       "   \n",
       "           [[ 0.5643,  0.3977,  0.0814,  ...,  0.3509,  0.4943, -0.1529],\n",
       "            [-0.6068,  0.9890,  0.0188,  ..., -0.4799,  0.3536, -0.0671],\n",
       "            [-0.3670,  1.1215,  0.4298,  ..., -0.0738,  1.2749, -0.4500],\n",
       "            ...,\n",
       "            [-0.4011,  1.4777, -0.3270,  ..., -0.7589,  0.5688, -0.2848],\n",
       "            [ 1.0066,  1.0317, -0.3675,  ...,  0.5589, -0.0053,  0.3826],\n",
       "            [-0.6056,  0.9864,  0.0159,  ..., -0.4789,  0.3526, -0.0667]],\n",
       "   \n",
       "           [[ 1.2110,  0.4743,  0.5688,  ...,  1.3883,  0.4872, -0.2569],\n",
       "            [-0.6057,  1.0315, -0.0822,  ..., -0.5174,  0.3563, -0.0924],\n",
       "            [-0.3489,  1.1706,  0.3236,  ..., -0.1131,  1.2951, -0.4650],\n",
       "            ...,\n",
       "            [-0.4045,  1.5202, -0.4293,  ..., -0.7981,  0.5848, -0.3075],\n",
       "            [-0.5116,  0.1958,  2.6732,  ..., -1.1401,  0.2646,  0.5761],\n",
       "            [-0.6055,  1.0284, -0.0822,  ..., -0.5169,  0.3567, -0.0915]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[-0.6098, -0.0953, -0.1541,  ...,  0.5174,  0.6142, -0.4692],\n",
       "            [-0.6070,  1.0034, -0.0071,  ..., -0.4877,  0.3606, -0.0921],\n",
       "            [-0.3623,  1.1312,  0.3914,  ..., -0.0760,  1.2942, -0.4683],\n",
       "            ...,\n",
       "            [-0.4025,  1.4905, -0.3534,  ..., -0.7702,  0.5790, -0.2991],\n",
       "            [ 0.5705,  0.3990,  0.0487,  ...,  0.3477,  0.5008, -0.1787],\n",
       "            [-0.6065,  1.0016, -0.0103,  ..., -0.4873,  0.3597, -0.0916]],\n",
       "   \n",
       "           [[ 1.2205,  0.4078,  0.1301,  ...,  0.1788, -1.2226,  1.2025],\n",
       "            [-0.5944,  0.9779, -0.0723,  ..., -0.5010,  0.3510, -0.0966],\n",
       "            [-0.3480,  1.1157,  0.3221,  ..., -0.0879,  1.2858, -0.4628],\n",
       "            ...,\n",
       "            [-0.3884,  1.4658, -0.4222,  ..., -0.7678,  0.5892, -0.3069],\n",
       "            [ 0.5719,  0.3908, -0.0191,  ...,  0.3419,  0.5041, -0.1877],\n",
       "            [-0.5939,  0.9769, -0.0733,  ..., -0.4983,  0.3503, -0.0992]],\n",
       "   \n",
       "           [[ 1.1832,  0.4896,  0.6305,  ...,  1.3848,  0.4871, -0.2691],\n",
       "            [-0.6258,  1.0424, -0.0303,  ..., -0.5232,  0.3630, -0.0955],\n",
       "            [ 0.8341, -2.4413,  0.3039,  ...,  0.2916, -0.9924, -1.7256],\n",
       "            ...,\n",
       "            [-0.4266,  1.5293, -0.3683,  ..., -0.8175,  0.5878, -0.3122],\n",
       "            [-0.2538, -0.1850, -0.7344,  ..., -0.4716,  1.0936,  1.8123],\n",
       "            [-0.6267,  1.0421, -0.0328,  ..., -0.5230,  0.3612, -0.0962]]],\n",
       "          device='cuda:0', grad_fn=<SliceBackward>)],\n",
       "  [tensor([[[ 1.2025,  0.4747,  0.6522,  ...,  1.4155,  0.4779, -0.2181],\n",
       "            [-0.6222,  1.0280, -0.0088,  ..., -0.4985,  0.3699, -0.0513],\n",
       "            [-0.3761,  1.1681,  0.4032,  ..., -0.0977,  1.2847, -0.4356],\n",
       "            ...,\n",
       "            [-0.4127,  1.5183, -0.3478,  ..., -0.7941,  0.5797, -0.2687],\n",
       "            [ 0.9969,  1.0722, -0.3944,  ...,  0.5261,  0.0170,  0.4011],\n",
       "            [-0.6215,  1.0269, -0.0113,  ..., -0.4951,  0.3709, -0.0535]],\n",
       "   \n",
       "           [[ 0.5643,  0.3977,  0.0814,  ...,  0.3509,  0.4943, -0.1529],\n",
       "            [-0.6068,  0.9890,  0.0188,  ..., -0.4799,  0.3536, -0.0671],\n",
       "            [-0.3670,  1.1215,  0.4298,  ..., -0.0738,  1.2749, -0.4500],\n",
       "            ...,\n",
       "            [-0.4011,  1.4777, -0.3270,  ..., -0.7589,  0.5688, -0.2848],\n",
       "            [ 1.0066,  1.0317, -0.3675,  ...,  0.5589, -0.0053,  0.3826],\n",
       "            [-0.6056,  0.9864,  0.0159,  ..., -0.4789,  0.3526, -0.0667]],\n",
       "   \n",
       "           [[ 1.2110,  0.4743,  0.5688,  ...,  1.3883,  0.4872, -0.2569],\n",
       "            [-0.6057,  1.0315, -0.0822,  ..., -0.5174,  0.3563, -0.0924],\n",
       "            [-0.3489,  1.1706,  0.3236,  ..., -0.1131,  1.2951, -0.4650],\n",
       "            ...,\n",
       "            [-0.4045,  1.5202, -0.4293,  ..., -0.7981,  0.5848, -0.3075],\n",
       "            [-0.5116,  0.1958,  2.6732,  ..., -1.1401,  0.2646,  0.5761],\n",
       "            [-0.6055,  1.0284, -0.0822,  ..., -0.5169,  0.3567, -0.0915]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[-0.6098, -0.0953, -0.1541,  ...,  0.5174,  0.6142, -0.4692],\n",
       "            [-0.6070,  1.0034, -0.0071,  ..., -0.4877,  0.3606, -0.0921],\n",
       "            [-0.3623,  1.1312,  0.3914,  ..., -0.0760,  1.2942, -0.4683],\n",
       "            ...,\n",
       "            [-0.4025,  1.4905, -0.3534,  ..., -0.7702,  0.5790, -0.2991],\n",
       "            [ 0.5705,  0.3990,  0.0487,  ...,  0.3477,  0.5008, -0.1787],\n",
       "            [-0.6065,  1.0016, -0.0103,  ..., -0.4873,  0.3597, -0.0916]],\n",
       "   \n",
       "           [[ 1.2205,  0.4078,  0.1301,  ...,  0.1788, -1.2226,  1.2025],\n",
       "            [-0.5944,  0.9779, -0.0723,  ..., -0.5010,  0.3510, -0.0966],\n",
       "            [-0.3480,  1.1157,  0.3221,  ..., -0.0879,  1.2858, -0.4628],\n",
       "            ...,\n",
       "            [-0.3884,  1.4658, -0.4222,  ..., -0.7678,  0.5892, -0.3069],\n",
       "            [ 0.5719,  0.3908, -0.0191,  ...,  0.3419,  0.5041, -0.1877],\n",
       "            [-0.5939,  0.9769, -0.0733,  ..., -0.4983,  0.3503, -0.0992]],\n",
       "   \n",
       "           [[ 1.1832,  0.4896,  0.6305,  ...,  1.3848,  0.4871, -0.2691],\n",
       "            [-0.6258,  1.0424, -0.0303,  ..., -0.5232,  0.3630, -0.0955],\n",
       "            [ 0.8341, -2.4413,  0.3039,  ...,  0.2916, -0.9924, -1.7256],\n",
       "            ...,\n",
       "            [-0.4266,  1.5293, -0.3683,  ..., -0.8175,  0.5878, -0.3122],\n",
       "            [-0.2538, -0.1850, -0.7344,  ..., -0.4716,  1.0936,  1.8123],\n",
       "            [-0.6267,  1.0421, -0.0328,  ..., -0.5230,  0.3612, -0.0962]]],\n",
       "          device='cuda:0', grad_fn=<SliceBackward>)]),\n",
       " (tensor([[[-0.1352,  0.0381, -0.3064, -0.5782],\n",
       "           [ 0.1914,  0.1031, -0.5510, -0.4200],\n",
       "           [-0.0044, -0.1323, -0.1424, -0.2040],\n",
       "           ...,\n",
       "           [ 0.8458, -0.0268, -0.3294,  0.0379],\n",
       "           [-0.0712,  0.2040,  0.3252,  0.3252],\n",
       "           [ 0.1920,  0.1035, -0.5506, -0.4200]],\n",
       "  \n",
       "          [[ 0.0416,  0.2831,  0.3287, -0.2501],\n",
       "           [ 0.1968,  0.1040, -0.5457, -0.4054],\n",
       "           [ 0.0019, -0.1301, -0.1346, -0.1890],\n",
       "           ...,\n",
       "           [ 0.8526, -0.0257, -0.3216,  0.0527],\n",
       "           [-0.0614,  0.2066,  0.3349,  0.3410],\n",
       "           [ 0.1972,  0.1042, -0.5455, -0.4054]],\n",
       "  \n",
       "          [[-0.1434,  0.0556, -0.3069, -0.5729],\n",
       "           [ 0.1875,  0.1181, -0.5513, -0.4131],\n",
       "           [-0.0098, -0.1193, -0.1443, -0.2025],\n",
       "           ...,\n",
       "           [ 0.8417, -0.0062, -0.3295,  0.0413],\n",
       "           [ 0.0560, -0.1226, -0.3426, -0.0498],\n",
       "           [ 0.1876,  0.1180, -0.5518, -0.4133]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 0.0290,  0.7494, -0.1202,  0.6721],\n",
       "           [ 0.1846,  0.1191, -0.5548, -0.4130],\n",
       "           [-0.0123, -0.1151, -0.1436, -0.2028],\n",
       "           ...,\n",
       "           [ 0.8392, -0.0080, -0.3299,  0.0438],\n",
       "           [ 0.0307,  0.3032,  0.3181, -0.2554],\n",
       "           [ 0.1849,  0.1190, -0.5546, -0.4128]],\n",
       "  \n",
       "          [[ 0.1934,  0.2152, -0.4604, -0.2121],\n",
       "           [ 0.1915,  0.1118, -0.5547, -0.4121],\n",
       "           [-0.0054, -0.1214, -0.1482, -0.1999],\n",
       "           ...,\n",
       "           [ 0.8476, -0.0157, -0.3316,  0.0437],\n",
       "           [ 0.0375,  0.2953,  0.3149, -0.2554],\n",
       "           [ 0.1919,  0.1114, -0.5542, -0.4118]],\n",
       "  \n",
       "          [[-0.1447,  0.0475, -0.3037, -0.5949],\n",
       "           [ 0.1842,  0.1109, -0.5487, -0.4317],\n",
       "           [ 0.3001,  0.2738, -0.5787, -0.1623],\n",
       "           ...,\n",
       "           [ 0.8409, -0.0150, -0.3288,  0.0217],\n",
       "           [ 0.2467, -0.2211, -0.2429, -0.2326],\n",
       "           [ 0.1843,  0.1106, -0.5485, -0.4327]]], device='cuda:0',\n",
       "         grad_fn=<AddBackward0>),\n",
       "  [tensor([[[ 1.2025,  0.4747,  0.6522,  ...,  1.4155,  0.4779, -0.2181],\n",
       "            [-0.6222,  1.0280, -0.0088,  ..., -0.4985,  0.3699, -0.0513],\n",
       "            [-0.3761,  1.1681,  0.4032,  ..., -0.0977,  1.2847, -0.4356],\n",
       "            ...,\n",
       "            [-0.4127,  1.5183, -0.3478,  ..., -0.7941,  0.5797, -0.2687],\n",
       "            [ 0.9969,  1.0722, -0.3944,  ...,  0.5261,  0.0170,  0.4011],\n",
       "            [-0.6215,  1.0269, -0.0113,  ..., -0.4951,  0.3709, -0.0535]],\n",
       "   \n",
       "           [[ 0.5643,  0.3977,  0.0814,  ...,  0.3509,  0.4943, -0.1529],\n",
       "            [-0.6068,  0.9890,  0.0188,  ..., -0.4799,  0.3536, -0.0671],\n",
       "            [-0.3670,  1.1215,  0.4298,  ..., -0.0738,  1.2749, -0.4500],\n",
       "            ...,\n",
       "            [-0.4011,  1.4777, -0.3270,  ..., -0.7589,  0.5688, -0.2848],\n",
       "            [ 1.0066,  1.0317, -0.3675,  ...,  0.5589, -0.0053,  0.3826],\n",
       "            [-0.6056,  0.9864,  0.0159,  ..., -0.4789,  0.3526, -0.0667]],\n",
       "   \n",
       "           [[ 1.2110,  0.4743,  0.5688,  ...,  1.3883,  0.4872, -0.2569],\n",
       "            [-0.6057,  1.0315, -0.0822,  ..., -0.5174,  0.3563, -0.0924],\n",
       "            [-0.3489,  1.1706,  0.3236,  ..., -0.1131,  1.2951, -0.4650],\n",
       "            ...,\n",
       "            [-0.4045,  1.5202, -0.4293,  ..., -0.7981,  0.5848, -0.3075],\n",
       "            [-0.5116,  0.1958,  2.6732,  ..., -1.1401,  0.2646,  0.5761],\n",
       "            [-0.6055,  1.0284, -0.0822,  ..., -0.5169,  0.3567, -0.0915]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[-0.6098, -0.0953, -0.1541,  ...,  0.5174,  0.6142, -0.4692],\n",
       "            [-0.6070,  1.0034, -0.0071,  ..., -0.4877,  0.3606, -0.0921],\n",
       "            [-0.3623,  1.1312,  0.3914,  ..., -0.0760,  1.2942, -0.4683],\n",
       "            ...,\n",
       "            [-0.4025,  1.4905, -0.3534,  ..., -0.7702,  0.5790, -0.2991],\n",
       "            [ 0.5705,  0.3990,  0.0487,  ...,  0.3477,  0.5008, -0.1787],\n",
       "            [-0.6065,  1.0016, -0.0103,  ..., -0.4873,  0.3597, -0.0916]],\n",
       "   \n",
       "           [[ 1.2205,  0.4078,  0.1301,  ...,  0.1788, -1.2226,  1.2025],\n",
       "            [-0.5944,  0.9779, -0.0723,  ..., -0.5010,  0.3510, -0.0966],\n",
       "            [-0.3480,  1.1157,  0.3221,  ..., -0.0879,  1.2858, -0.4628],\n",
       "            ...,\n",
       "            [-0.3884,  1.4658, -0.4222,  ..., -0.7678,  0.5892, -0.3069],\n",
       "            [ 0.5719,  0.3908, -0.0191,  ...,  0.3419,  0.5041, -0.1877],\n",
       "            [-0.5939,  0.9769, -0.0733,  ..., -0.4983,  0.3503, -0.0992]],\n",
       "   \n",
       "           [[ 1.1832,  0.4896,  0.6305,  ...,  1.3848,  0.4871, -0.2691],\n",
       "            [-0.6258,  1.0424, -0.0303,  ..., -0.5232,  0.3630, -0.0955],\n",
       "            [ 0.8341, -2.4413,  0.3039,  ...,  0.2916, -0.9924, -1.7256],\n",
       "            ...,\n",
       "            [-0.4266,  1.5293, -0.3683,  ..., -0.8175,  0.5878, -0.3122],\n",
       "            [-0.2538, -0.1850, -0.7344,  ..., -0.4716,  1.0936,  1.8123],\n",
       "            [-0.6267,  1.0421, -0.0328,  ..., -0.5230,  0.3612, -0.0962]]],\n",
       "          device='cuda:0', grad_fn=<SliceBackward>)],\n",
       "  [tensor([[[ 1.2025,  0.4747,  0.6522,  ...,  1.4155,  0.4779, -0.2181],\n",
       "            [-0.6222,  1.0280, -0.0088,  ..., -0.4985,  0.3699, -0.0513],\n",
       "            [-0.3761,  1.1681,  0.4032,  ..., -0.0977,  1.2847, -0.4356],\n",
       "            ...,\n",
       "            [-0.4127,  1.5183, -0.3478,  ..., -0.7941,  0.5797, -0.2687],\n",
       "            [ 0.9969,  1.0722, -0.3944,  ...,  0.5261,  0.0170,  0.4011],\n",
       "            [-0.6215,  1.0269, -0.0113,  ..., -0.4951,  0.3709, -0.0535]],\n",
       "   \n",
       "           [[ 0.5643,  0.3977,  0.0814,  ...,  0.3509,  0.4943, -0.1529],\n",
       "            [-0.6068,  0.9890,  0.0188,  ..., -0.4799,  0.3536, -0.0671],\n",
       "            [-0.3670,  1.1215,  0.4298,  ..., -0.0738,  1.2749, -0.4500],\n",
       "            ...,\n",
       "            [-0.4011,  1.4777, -0.3270,  ..., -0.7589,  0.5688, -0.2848],\n",
       "            [ 1.0066,  1.0317, -0.3675,  ...,  0.5589, -0.0053,  0.3826],\n",
       "            [-0.6056,  0.9864,  0.0159,  ..., -0.4789,  0.3526, -0.0667]],\n",
       "   \n",
       "           [[ 1.2110,  0.4743,  0.5688,  ...,  1.3883,  0.4872, -0.2569],\n",
       "            [-0.6057,  1.0315, -0.0822,  ..., -0.5174,  0.3563, -0.0924],\n",
       "            [-0.3489,  1.1706,  0.3236,  ..., -0.1131,  1.2951, -0.4650],\n",
       "            ...,\n",
       "            [-0.4045,  1.5202, -0.4293,  ..., -0.7981,  0.5848, -0.3075],\n",
       "            [-0.5116,  0.1958,  2.6732,  ..., -1.1401,  0.2646,  0.5761],\n",
       "            [-0.6055,  1.0284, -0.0822,  ..., -0.5169,  0.3567, -0.0915]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[-0.6098, -0.0953, -0.1541,  ...,  0.5174,  0.6142, -0.4692],\n",
       "            [-0.6070,  1.0034, -0.0071,  ..., -0.4877,  0.3606, -0.0921],\n",
       "            [-0.3623,  1.1312,  0.3914,  ..., -0.0760,  1.2942, -0.4683],\n",
       "            ...,\n",
       "            [-0.4025,  1.4905, -0.3534,  ..., -0.7702,  0.5790, -0.2991],\n",
       "            [ 0.5705,  0.3990,  0.0487,  ...,  0.3477,  0.5008, -0.1787],\n",
       "            [-0.6065,  1.0016, -0.0103,  ..., -0.4873,  0.3597, -0.0916]],\n",
       "   \n",
       "           [[ 1.2205,  0.4078,  0.1301,  ...,  0.1788, -1.2226,  1.2025],\n",
       "            [-0.5944,  0.9779, -0.0723,  ..., -0.5010,  0.3510, -0.0966],\n",
       "            [-0.3480,  1.1157,  0.3221,  ..., -0.0879,  1.2858, -0.4628],\n",
       "            ...,\n",
       "            [-0.3884,  1.4658, -0.4222,  ..., -0.7678,  0.5892, -0.3069],\n",
       "            [ 0.5719,  0.3908, -0.0191,  ...,  0.3419,  0.5041, -0.1877],\n",
       "            [-0.5939,  0.9769, -0.0733,  ..., -0.4983,  0.3503, -0.0992]],\n",
       "   \n",
       "           [[ 1.1832,  0.4896,  0.6305,  ...,  1.3848,  0.4871, -0.2691],\n",
       "            [-0.6258,  1.0424, -0.0303,  ..., -0.5232,  0.3630, -0.0955],\n",
       "            [ 0.8341, -2.4413,  0.3039,  ...,  0.2916, -0.9924, -1.7256],\n",
       "            ...,\n",
       "            [-0.4266,  1.5293, -0.3683,  ..., -0.8175,  0.5878, -0.3122],\n",
       "            [-0.2538, -0.1850, -0.7344,  ..., -0.4716,  1.0936,  1.8123],\n",
       "            [-0.6267,  1.0421, -0.0328,  ..., -0.5230,  0.3612, -0.0962]]],\n",
       "          device='cuda:0', grad_fn=<SliceBackward>)]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_acc(input:Tensor, t1:Tensor, t2:Tensor)->Rank0Tensor:\n",
    "    n = t1.shape[0]\n",
    "    input = input[0].argmax(dim=-1).view(n,-1)\n",
    "    t1 = t1.view(n,-1)\n",
    "    mask = t1 != vocab.pad_idx\n",
    "    return (input[mask]==t1[mask]).float().mean()\n",
    "\n",
    "def ns_acc(input:Tensor, t1:Tensor, t2:Tensor)->Rank0Tensor:\n",
    "    return accuracy(input[1], t2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics = [mask_acc, ns_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.722114, tensor(0.0998), tensor(0.3690)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='3', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      33.33% [1/3 02:12<04:25]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mask_acc</th>\n",
       "      <th>ns_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.142874</td>\n",
       "      <td>3.246043</td>\n",
       "      <td>0.536631</td>\n",
       "      <td>0.467248</td>\n",
       "      <td>02:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='807', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-0ec2ff9e12e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/fastai/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     21\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/midi/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/midi/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(3, 1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
