{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.text import *\n",
    "from enum import Enum\n",
    "import torch\n",
    "from fastai.text.models.awd_lstm import *\n",
    "from fastai.text.models.transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=10, threshold=40, linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "from src.fastai_data import *\n",
    "from src.encode_data import *\n",
    "from src.serve import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.music_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.models.transformer import _line_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = v15s_config(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['mem_len'] = 0\n",
    "config['mask'] = False\n",
    "config['bs'] = 4\n",
    "config['ctx_len'] = 1024\n",
    "config['bptt'] = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _model_meta[MusicTransformer] = _model_meta[TransformerXL]\n",
    "# _model_meta[MusicTransformer]['config_lm'] = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctx_len': 1024,\n",
       " 'n_layers': 16,\n",
       " 'n_heads': 8,\n",
       " 'd_model': 256,\n",
       " 'd_head': 32,\n",
       " 'd_inner': 2048,\n",
       " 'resid_p': 0.1,\n",
       " 'attn_p': 0.1,\n",
       " 'ff_p': 0.1,\n",
       " 'embed_p': 0.1,\n",
       " 'output_p': 0.1,\n",
       " 'bias': False,\n",
       " 'scale': True,\n",
       " 'act': <Activation.GeLU: 3>,\n",
       " 'double_drop': True,\n",
       " 'tie_weights': True,\n",
       " 'out_bias': True,\n",
       " 'init': <function fastai.text.models.transformer.init_transformer(m)>,\n",
       " 'mem_len': 0,\n",
       " 'mask': False,\n",
       " 'pad_idx': 1,\n",
       " 'bos_idx': 0,\n",
       " 'sep_idx': 8,\n",
       " 'transpose_range': (0, 12),\n",
       " 'note_range': (9, 138),\n",
       " 'bs': 4,\n",
       " 'bptt': 1024,\n",
       " 'vocab_size': 274}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUGGING PARAMS\n",
    "config['vocab_size'] = 300\n",
    "config['attn_p'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaskType = Enum('TaskType', 'MaskOnly, NextSent, Translate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Transform\n",
    "def next_sentence_ranges(x, y, max_cls=4):\n",
    "    bs,bptt = x.shape\n",
    "    s = min(random.randint(1, max_cls), bs-2)\n",
    "    \n",
    "    min_seq_len = bptt // s\n",
    "\n",
    "    bs_shift = [0]+(np.random.choice(bs-1, s, replace=False)+1).tolist()\n",
    "    row_shift = [int(min_seq_len + random.randint(-min_seq_len, min_seq_len)//s) for i in range(s)]\n",
    "    \n",
    "    accum = 0\n",
    "    ranges = []\n",
    "    for i in range(s):\n",
    "        end = accum + row_shift[i] if i < (s-1) else bptt\n",
    "        ranges.append((i, bs_shift[i], accum, end))\n",
    "        accum = end\n",
    "    return ranges\n",
    "\n",
    "def next_sentence_tfm(b, max_cls=4):\n",
    "    x, y = b\n",
    "    x_new = x.clone()\n",
    "    y_new = y.clone()\n",
    "    z = torch.zeros_like(x)\n",
    "    ranges = next_sentence_ranges(x, y, max_cls)\n",
    "    for i,shift,s,e in ranges:\n",
    "        if i == 0: continue\n",
    "        x_new[:, s:e] = torch.roll(x, shifts=shift, dims=0)[:, s:e]\n",
    "        y_new[:, s:e] = torch.roll(y, shifts=shift, dims=0)[:, s:e]\n",
    "        z[:, s:e] = i\n",
    "    return (x_new, TaskType.NextSent.value), (y_new, z)\n",
    "\n",
    "def mask_tfm(b, word_range=vocab.npenc_range, pad_idx=vocab.pad_idx, mask_idx=vocab.mask_idx, p=0.2, double=False, mask_last=False):\n",
    "    # p = replacement probability\n",
    "    # double = mask 2 sequences at once\n",
    "    # y is ignored\n",
    "#     y = x.clone()\n",
    "    x,y = b\n",
    "    rand = torch.rand(x.shape, device=x.device)\n",
    "    rand[x < word_range[0]] = 1.0\n",
    "    if mask_last: rand[-1] = 0.0\n",
    "    y[rand > p] = pad_idx\n",
    "    x[rand <= (p*.8)] = mask_idx # 80% = mask\n",
    "    wrong_word = (rand > (p*.8)) & (rand <= (p*.9)) # 10% = wrong word\n",
    "    x[wrong_word] = torch.randint(*word_range, [wrong_word.sum().item()], device=x.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = Path('../../data/midi/v15/piano_duet/')\n",
    "# dl_tfms = [mask_tfm, next_sentence_tfm]\n",
    "# data = load_music_data(path, cache_name='tmp/sample', vocab=vocab, y_offset=0, dl_tfms=dl_tfms, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2S Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_tempo(t, sep_idx=0):\n",
    "    avg = t[t[:, 0] == sep_idx][:, 1].sum()/t.shape[0]\n",
    "    return 'mt'+str(int(max(round(avg), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMEMBER TO CHECK IF x,y ARE WITHIN RANGE AND HAVE ENOUGH DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2SPreloader(Callback):\n",
    "    def __init__(self, dataset:LabelList, bptt:int=512, **kwargs):\n",
    "        self.dataset,self.bptt = dataset,bptt\n",
    "        self.vocab = vocab\n",
    "        self.single_tfm = partial(to_single_stream, vocab=vocab)\n",
    "        self.transpose_tfm = partial(rand_transpose, note_range=vocab.note_range, rand_range=(0,12))\n",
    "    \n",
    "    def __getitem__(self, k:int):\n",
    "        item,_ = self.dataset[k]\n",
    "        x,y = item\n",
    "        \n",
    "        \n",
    "        # LETS DO A CHECK HERE\n",
    "        # MAKE SURE TO REMOVE THIS AFTER DATA IS RE-ENCODED\n",
    "        x[x > 128] = 128\n",
    "        y[y > 128] = 128\n",
    "        \n",
    "        melody_meta = np.array([self.vocab.stoi[MSEQ], self.vocab.stoi[avg_tempo(x)]]) # pad should be average notes - tempo\n",
    "        chord_meta = np.array([self.vocab.stoi[CSEQ], self.vocab.stoi[avg_tempo(y)]])\n",
    "        x = self.single_tfm(x, start_seq=melody_meta)\n",
    "        y = self.single_tfm(y, start_seq=chord_meta)\n",
    "        \n",
    "        x,y = self.transpose_tfm((x,y))\n",
    "        \n",
    "        x = np.pad(x, (0,max(0,self.bptt-len(x))), 'constant', constant_values=vocab.pad_idx)[:self.bptt]\n",
    "        y_offset = 1\n",
    "        y = np.pad(y, (y_offset,max(0,self.bptt-len(y))), 'constant', constant_values=vocab.pad_idx)[:self.bptt+1]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preloader itself contains all the transforms\n",
    "def mask_s2s_tfm(b, word_range=vocab.npenc_range, pad_idx=vocab.pad_idx, \n",
    "             mask_idx=vocab.mask_idx, p=0.1, double=False, mask_last=False):\n",
    "    x,y_s2s = b\n",
    "    x_mask,y_mask = mask_tfm((x,x.clone()))\n",
    "    return (x,TaskType.Translate.value,y_s2s[:,:-1]),(y_mask,y_s2s[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_tfms = [mask_s2s_tfm]\n",
    "\n",
    "path = Path('../../data/midi/v15/s2s_encode/')\n",
    "data = MusicDataBunch.load(path, bs=config['bs'], cache_name='tmp/hook', preloader_cls=S2SPreloader, dl_tfms=[mask_s2s_tfm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_len = 0\n",
    "# x_len = 16 # bptt\n",
    "# seq_len = m_len+x_len\n",
    "# torch.triu(torch.ones(x_len, seq_len), diagonal=m_len).byte()[None,None].cpu().numpy()\n",
    "# torch.triu(torch.ones(x_len, seq_len), diagonal=m_len+1).byte()[None,None].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    \"Embedding + positional encoding + dropout\"\n",
    "    def __init__(self, vocab_sz:int, emb_sz:int, inp_p:float=0.):\n",
    "        super().__init__()\n",
    "        self.emb_sz = emb_sz\n",
    "        self.embed = embedding(vocab_sz, emb_sz)\n",
    "        self.pos_enc = PositionalEncoding(emb_sz)\n",
    "        self.drop = nn.Dropout(inp_p)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        pos = torch.arange(0, inp.size(1), device=inp.device).float()\n",
    "        return self.drop(self.embed(inp)), self.pos_enc(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTransformer(nn.Module):\n",
    "    \"TransformerXL model: https://arxiv.org/abs/1901.02860.\"\n",
    "    def __init__(self, embed:nn.Module, n_layers:int, n_heads:int, d_model:int, d_head:int, d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., bias:bool=False, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadRelativeAttention,\n",
    "                 learned_pos_enc:bool=False, mask:bool=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = embed\n",
    "        self.u = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.v = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.n_layers,self.d_model,self.mask = n_layers,d_model,mask\n",
    "        self.layers = nn.ModuleList([DecoderLayer(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                      ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "                      attn_cls=attn_cls) for k in range(n_layers)])\n",
    "        \n",
    "        nn.init.normal_(self.u, 0., 0.02)\n",
    "        nn.init.normal_(self.v, 0., 0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs,x_len = x.size()\n",
    "        inp, pos_enc = self.encoder(x)\n",
    "\n",
    "        mask = lm_mask(x_len, x.device) if self.mask else None\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            inp = layer(inp, r=pos_enc, u=self.u, v=self.v, mask=mask, mem=None)\n",
    "        core_out = inp[:,-x_len:]\n",
    "        return core_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertHead(nn.Module):\n",
    "    def __init__(self, encoder, mask_decoder, ns_decoder, s2s_decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.mask_decoder = mask_decoder\n",
    "        self.ns_decoder = ns_decoder\n",
    "        self.s2s_decoder = s2s_decoder\n",
    "        \n",
    "    def forward(self, x, task_type=None, y=None):\n",
    "#         x_emb = self.embed(x)\n",
    "        x_enc = self.encoder(x)\n",
    "        \n",
    "        if task_type == TaskType.NextSent.value: # mask, and next sentence task\n",
    "            return self.mask_decoder(x_enc), task_type, self.ns_decoder(x_enc)\n",
    "        if task_type == TaskType.Translate.value:\n",
    "            return self.mask_decoder(x_enc), task_type, self.s2s_decoder(x_enc, y)\n",
    "        return self.mask_decoder(x_enc), task_type\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.encoder, self.mask_decoder, self.ns_decoder, self.s2s_decoder][idx]\n",
    "        \n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_mask(x_len, device, m_len=0, size=(1,1)):\n",
    "    win_size,k = size\n",
    "    mem_mask = np.zeros((x_len,m_len))\n",
    "    tri_mask = np.triu(np.ones((x_len//win_size+1,x_len//win_size+1)),k=k)\n",
    "    window_mask = tri_mask.repeat(win_size,axis=0).repeat(win_size,axis=1)[:x_len,:x_len]\n",
    "    np_mask = np.concatenate((mem_mask, window_mask), axis=1)\n",
    "    mask = torch.tensor(np_mask, device=device).byte()[None,None]\n",
    "    return mask\n",
    "    \n",
    "def rand_window_mask(x_len,m_len,device,max_size=3,p=0.2,is_eval=False):\n",
    "    if is_eval or m_len == 0 or np.random.rand() >= p: \n",
    "        win_size,k = (1,1)\n",
    "    else: win_size,k = (np.random.randint(0,max_size)+1,0)\n",
    "    return window_mask(x_len, device, m_len, size=(win_size,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_mask(x_len, device):\n",
    "    return torch.triu(torch.ones((x_len, x_len), device=device), diagonal=1)[None,None].byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KVMultiHeadRelativeAttention(nn.Module):\n",
    "    \"MutiHeadAttention with relative positional encoding.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., attn_p:float=0., bias:bool=True,\n",
    "                 scale:bool=True):\n",
    "        super().__init__()\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
    "        \n",
    "        self.q_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.k_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.v_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        \n",
    "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
    "        self.drop_att,self.drop_res = nn.Dropout(attn_p),nn.Dropout(resid_p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.r_attn = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        \n",
    "    def forward(self, q:Tensor, k:Tensor, v:Tensor, \n",
    "                r:Tensor=None, g_u:Tensor=None, g_v:Tensor=None, \n",
    "                mask:Tensor=None, **kwargs):\n",
    "        return self.ln(q + self.drop_res(self.out(self._apply_attention(q, k, v, r, g_u, g_v, mask=mask, **kwargs))))\n",
    "    \n",
    "    def _apply_attention(self, q:Tensor, k:Tensor, v:Tensor, \n",
    "                         r:Tensor=None, g_u:Tensor=None, g_v:Tensor=None, \n",
    "                         mask:Tensor=None):\n",
    "        #Notations from the paper: x input, r vector of relative distance between two elements, u et v learnable\n",
    "        #parameters of the model common between all layers, mask to avoid cheating and mem the previous hidden states.\n",
    "        bs,x_len,seq_len = q.size(0),q.size(1),r.size(0)\n",
    "        wq,wk,wv = self.q_wgt(q),self.k_wgt(k),self.v_wgt(v)\n",
    "        wq = wq[:,-x_len:]\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        wkr = self.r_attn(r)\n",
    "        wkr = wkr.view(seq_len, self.n_heads, self.d_head)\n",
    "        wkr = wkr.permute(1,2,0)\n",
    "        #### compute attention score (AC is (a) + (c) and BS is (b) + (d) in the paper)\n",
    "        AC = torch.matmul(wq+g_u,wk)\n",
    "        BD = _line_shift(torch.matmul(wq+g_v, wkr))\n",
    "        if self.scale: attn_score = (AC + BD).mul_(1/(self.d_head ** 0.5))\n",
    "        if mask is not None: \n",
    "            attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return attn_vec.permute(0, 2, 1, 3).contiguous().view(bs, x_len, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2SDecoderBlock(nn.Module):\n",
    "    \"Decoder block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0., ff_p:float=0.,\n",
    "                 bias:bool=True, scale:bool=True, double_drop:bool=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha1 = KVMultiHeadRelativeAttention(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.mha2 = KVMultiHeadRelativeAttention(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.ff   = feed_forward(d_model, d_inner, ff_p=ff_p, double_drop=double_drop)\n",
    "    \n",
    "    def forward(self, targ:Tensor, enc:Tensor, \n",
    "                r=None, u=None, v=None,\n",
    "                mask_in:Tensor=None, mask_out:Tensor=None): \n",
    "        y = self.mha1(targ, targ, targ, r, u, v, mask=mask_out)\n",
    "        return self.ff(self.mha2(y, enc, enc, r, u, v, mask=mask_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2SDecoder(nn.Module):\n",
    "    def __init__(self, embed:nn.Module, n_hid:int, vocab_sz:int, n_layers:int, n_heads:int, d_model:int, d_head:int, d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., bias:bool=False, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadRelativeAttention,\n",
    "                 learned_pos_enc:bool=False, mask:bool=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = embed\n",
    "        self.u = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.v = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.n_layers,self.d_model,self.mask = n_layers,d_model,mask\n",
    "        self.layers = nn.ModuleList([S2SDecoderBlock(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                      ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "                      attn_cls=attn_cls) for k in range(n_layers)])\n",
    "        self.head = MusicLinearDecoder(d_model, vocab_sz, tie_encoder=embed.embed, **kwargs)\n",
    "    \n",
    "        nn.init.normal_(self.u, 0., 0.02)\n",
    "        nn.init.normal_(self.v, 0., 0.02)\n",
    "        \n",
    "    def forward(self, enc, targ):\n",
    "        # x = encoder, y = target\n",
    "        bs,targ_len = targ.size()\n",
    "        \n",
    "        targ_emb, pos_enc = self.encoder(targ)\n",
    "\n",
    "#         mask = window_mask(x_len, x.device) if self.mask else None\n",
    "        mask_out = lm_mask(targ_len, targ.device)\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            targ_emb = layer(targ_emb, enc, mask_out=mask_out,\n",
    "                        r=pos_enc, u=self.u, v=self.v)\n",
    "        return self.head(targ_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MusicLinearDecoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_hid:int, n_out:int, output_p:float, tie_encoder:nn.Module=None, bias:bool=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        output = self.output_dp(input)\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_music_model(vocab_sz:int, config:dict=None, drop_mult:float=1.):\n",
    "    \"Create a language model from `arch` and its `config`, maybe `pretrained`.\"\n",
    "    for k in config.keys(): \n",
    "        if k.endswith('_p'): config[k] *= drop_mult\n",
    "#     tie_weights,output_p,out_bias = map(config.pop, ['tie_weights', 'output_p', 'out_bias'])\n",
    "    tie_weights,output_p,out_bias = map(config.get, ['tie_weights', 'output_p', 'out_bias'])\n",
    "    n_hid = config['d_model']\n",
    "    embed = TransformerEmbedding(vocab_sz, n_hid, inp_p=config['embed_p'])\n",
    "    encoder = MusicTransformer(embed=embed, **config)\n",
    "    mask_decoder = MusicLinearDecoder(n_hid, vocab_sz, output_p, tie_encoder=embed.embed, bias=out_bias)\n",
    "    ns_decoder = MusicLinearDecoder(n_hid, 4, output_p, tie_encoder=None, bias=out_bias)\n",
    "    s2s_decoder = S2SDecoder(embed, n_hid, vocab_sz, **config)\n",
    "    model = BertHead(encoder, mask_decoder, ns_decoder, s2s_decoder)\n",
    "    return model.apply(init_transformer)\n",
    "\n",
    "\n",
    "def music_model_learner(data:DataBunch, config:dict=None, drop_mult:float=1., pretrained:bool=False,\n",
    "                        pretrained_fnames:OptStrTuple=None, **learn_kwargs) -> 'LanguageLearner':\n",
    "    \"Create a `Learner` with a language model from `data` and `arch`.\"\n",
    "    model = get_music_model(config['vocab_size'], config=config, drop_mult=drop_mult)\n",
    "    learn = MusicLearner(data, model, split_func=tfmerXL_lm_split, \n",
    "                         bos_idx=config['bos_idx'], sep_idx=config['sep_idx'],\n",
    "                        **learn_kwargs)\n",
    "    \n",
    "    if pretrained:\n",
    "        if 'url' not in meta: \n",
    "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "            return learn\n",
    "        model_path = untar_data(meta['url'], data=False)\n",
    "        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        learn.load_pretrained(*fnames)\n",
    "        learn.freeze()\n",
    "    if pretrained_fnames is not None:\n",
    "        fnames = [learn.path/learn.model_dir/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]\n",
    "        learn.load_pretrained(*fnames)\n",
    "        learn.freeze()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sep_idx: 8\n"
     ]
    }
   ],
   "source": [
    "learn = music_model_learner(data, config.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = data.one_batch(cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  6,   4,   8, 141,  61, 140,   8, 140,   4,   4,   8, 140,  51, 140,\n",
       "            8, 140,  59, 140,   8, 140,  58, 140,   8, 140,   4, 140,   8,   4,\n",
       "           59, 140,   4, 140,  58, 140,   8, 140,  51,   4,   8, 142,   4, 140,\n",
       "            4, 140,  60, 140,   8, 140,  61,   4,   8, 140,  67,   4,   1,   1,\n",
       "            1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
       "         [  6, 273,   8, 140,  78, 140,   8, 140,  75, 140,   8, 140,   4,   4,\n",
       "            8, 141,  73,   4,   8,   4,  73,   4,   8,   4,  70,   4,   8, 140,\n",
       "            4, 141,   8, 141,  68, 141,   8, 142,  75, 141,   8, 141,  72, 140,\n",
       "            8, 140,   4, 141,   8,   4,  70, 141,   8, 142,  70, 141,   8, 141,\n",
       "           67, 140,   8, 140,  63,   4,   8,   4,  65, 141,   8, 142,  78, 141],\n",
       "         [  6, 273,   8,   4,   4, 143,   8, 143,   4,   4,   8, 143,   4, 143,\n",
       "            4,   4,  69, 119,   8, 143,  69, 141,   8, 141,  71, 140,   8, 140,\n",
       "           69, 140,   4, 140,  66, 141,   8, 141,   4, 141,   8, 141,   4,   4,\n",
       "            8, 147,  66, 141,   8, 141,  69, 141,   8, 141,  71,   4,   8,   4,\n",
       "           76, 143,   8, 143,  73, 143,   8, 143,  74, 143,   8,   4,  73, 141],\n",
       "         [  6, 273,   8, 140,  71, 143,   8, 143,  71, 142,   8, 142,  69, 140,\n",
       "            8, 140,  71,   4,   8, 141,  74,  15,   8, 141,  74, 141,   4, 141,\n",
       "           76, 141,   8, 141,  71, 143,   8, 143,  71, 142,   8, 142,   4,   4,\n",
       "            4, 140,  71,   4,   8, 141,   4, 141,   8, 141,  66, 141,   8, 141,\n",
       "           69, 141,   8, 141,  71, 143,   8,   4,  71, 142,   8,   4,  69, 140]],\n",
       "        device='cuda:0'),\n",
       " 3,\n",
       " tensor([[  1,   5, 273,   8, 156,  65, 155,  61, 155,  58, 155,   8, 155,  66,\n",
       "          147,  63, 147,  61, 147,  58, 147,   8, 147,  65, 147,  61, 147,  58,\n",
       "          147,   8, 147,  56, 163,  52, 163,  49, 163,   8, 163,  58, 143,  54,\n",
       "          143,  51, 143,  49, 143,   8, 143,  60, 143,  56, 143,  53, 143,  51,\n",
       "          143,   8, 143,  61, 155,  57, 155,  54, 155,   8, 155,  68, 147,  66],\n",
       "         [  1,   5, 273,   8, 140,  71, 145,  68, 145,  64, 145,  58, 145,  54,\n",
       "          145,   8, 145,  66, 149,  63, 149,  58, 149,  56, 149,   8, 149,  68,\n",
       "          145,  65, 145,  61, 145,  55, 145,  51, 145,   8, 145,  67, 149,  63,\n",
       "          149,  60, 149,  56, 149,   8, 149,  71, 145,  68, 145,  64, 145,  58,\n",
       "          145,  54, 145,   8, 145,  66, 149,  63, 149,  58, 149,  56, 149,   8],\n",
       "         [  1,   5, 273,   8, 140,  62, 155,  59, 155,  55, 155,   8, 155,  69,\n",
       "          155,  66, 155,  62, 155,   8, 155,  62, 147,  59, 147,  55, 147,   8,\n",
       "          147,  64, 147,  61, 147,  57, 147,   8, 147,  69, 155,  66, 155,  62,\n",
       "          155,  59, 155,   8, 155,  62, 155,  59, 155,  55, 155,   8, 155,  69,\n",
       "          155,  66, 155,  62, 155,   8, 155,  62, 155,  59, 155,  56, 155,  52],\n",
       "         [  1,   5, 273,   8, 140,  66, 155,  62, 155,  59, 155,   8, 155,  62,\n",
       "          147,  59, 147,  55, 147,   8, 147,  64, 147,  61, 147,  57, 147,   8,\n",
       "          147,  66, 155,  62, 155,  59, 155,   8, 155,  62, 147,  59, 147,  55,\n",
       "          147,   8, 147,  64, 147,  62, 147,  57, 147,   8, 147,  66, 155,  62,\n",
       "          155,  59, 155,   8, 155,  62, 147,  59, 147,  55, 147,   8, 147,  64]],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 70])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLoss():\n",
    "    def __init__(self, mask_loss, sent_loss, s2s_loss):\n",
    "        self.mask_loss = mask_loss\n",
    "        self.sent_loss = sent_loss\n",
    "        self.s2s_loss = s2s_loss\n",
    "        \n",
    "    def __call__(self, input:Tensor, target:Tensor, target_2:Tensor, **kwargs)->Rank0Tensor:\n",
    "        x_mask, task_type, x_task = input\n",
    "        m = self.mask_loss.__call__(x_mask, target, **kwargs)\n",
    "        \n",
    "        if task_type == TaskType.NextSent.value: s = self.sent_loss.__call__(x_task, target_2, **kwargs)\n",
    "        elif task_type == TaskType.Translate.value: s = self.s2s_loss.__call__(x_task, target_2, **kwargs)\n",
    "        else: s = 0\n",
    "\n",
    "        return m + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTrainer(LearnerCallback):\n",
    "    \"`Callback` that regroups lr adjustment to seq_len, AR and TAR.\"\n",
    "    def __init__(self, learn:Learner):\n",
    "        super().__init__(learn)\n",
    "        \n",
    "#     def on_loss_begin(self, last_output:Tuple[Tensor,Tensor,Tensor], **kwargs):\n",
    "#         \"Save the extra outputs for later and only returns the true output.\"\n",
    "#         return {'last_output': (last_output[0], last_output[1]) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.callbacks = [BertTrainer(learn, alpha=2, beta=1)]\n",
    "learn.callbacks = [BertTrainer(learn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = BertLoss(CrossEntropyFlat(ignore_index=vocab.pad_idx), CrossEntropyFlat(), CrossEntropyFlat(ignore_index=vocab.pad_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 3.8807e-01, -2.4387e-01,  8.2410e-02,  ..., -6.1430e-02,\n",
       "            2.8012e-01, -2.7947e-01],\n",
       "          [-3.1593e-01, -4.9631e-01,  4.5751e-02,  ...,  2.6268e-01,\n",
       "            1.8078e-01,  3.9286e-03],\n",
       "          [ 5.0875e-01, -1.6387e-01, -1.1938e-02,  ...,  1.7764e-01,\n",
       "           -1.0685e-03,  1.5366e-01],\n",
       "          ...,\n",
       "          [ 1.8689e-01,  4.3111e+00,  1.6423e-01,  ..., -5.1953e-02,\n",
       "            6.2069e-01, -5.5244e-01],\n",
       "          [-1.5063e-02,  3.9731e+00, -1.1039e-01,  ..., -3.1504e-01,\n",
       "            4.4508e-01, -5.4335e-01],\n",
       "          [-1.6639e-01,  4.2005e+00, -1.2901e-01,  ...,  2.4151e-02,\n",
       "            4.5935e-01, -1.2950e+00]],\n",
       " \n",
       "         [[-1.6822e-02,  4.8141e-02,  1.3287e-01,  ..., -3.1430e-03,\n",
       "            2.7192e-01, -7.6868e-01],\n",
       "          [ 1.1095e-01,  3.6592e-01, -3.1452e-01,  ...,  2.6955e-01,\n",
       "            2.6242e-01, -2.9980e-01],\n",
       "          [ 2.2107e-01, -4.9531e-01,  1.2466e-01,  ...,  4.1153e-01,\n",
       "            3.2722e-02,  3.0553e-01],\n",
       "          ...,\n",
       "          [ 1.8581e-01,  9.1633e-02,  3.1785e-01,  ..., -3.8439e-01,\n",
       "            2.3620e-02,  1.9516e-01],\n",
       "          [-3.7539e-01, -1.0240e-01, -1.1558e-01,  ...,  5.4437e-02,\n",
       "            3.8254e-01,  6.6375e-01],\n",
       "          [ 5.1010e-02, -1.0491e-01,  1.3171e-01,  ...,  1.3030e-01,\n",
       "           -2.1539e-02,  8.7055e-02]],\n",
       " \n",
       "         [[ 1.5985e-01,  7.5273e-02, -2.0937e-02,  ...,  1.0215e-01,\n",
       "            3.4752e-01, -4.6974e-02],\n",
       "          [-1.3199e-03,  2.2707e-01, -4.8912e-01,  ...,  5.3732e-02,\n",
       "            3.4201e-01, -5.0188e-01],\n",
       "          [ 3.2410e-01, -2.2405e-02,  1.0035e-01,  ...,  6.6628e-01,\n",
       "           -6.0956e-02,  2.0092e-01],\n",
       "          ...,\n",
       "          [-3.3342e-01, -2.7482e-01, -2.0871e-01,  ...,  6.9871e-02,\n",
       "           -2.1665e-01, -8.2091e-02],\n",
       "          [-9.4540e-02, -3.4902e-02, -4.7712e-01,  ...,  2.6730e-01,\n",
       "            5.7161e-01, -3.0556e-01],\n",
       "          [-2.3610e-01, -4.5004e-01,  9.3619e-02,  ...,  5.0970e-01,\n",
       "            3.1634e-01,  3.0494e-01]],\n",
       " \n",
       "         [[ 4.4943e-01, -3.3925e-01, -2.4813e-02,  ..., -1.7007e-01,\n",
       "           -1.2035e-01, -2.7733e-01],\n",
       "          [-4.6334e-02,  1.9064e-01, -3.5955e-01,  ...,  1.7713e-01,\n",
       "           -8.0758e-02, -4.1517e-02],\n",
       "          [ 1.3850e-01, -1.5764e-01, -1.1900e-01,  ...,  3.9724e-01,\n",
       "            2.3846e-01, -2.2464e-02],\n",
       "          ...,\n",
       "          [-2.2379e-01, -4.9103e-01, -2.8006e-01,  ...,  1.3405e-01,\n",
       "            4.2402e-01,  6.6786e-02],\n",
       "          [-2.5227e-01, -9.6136e-02,  5.7559e-02,  ...,  2.6585e-01,\n",
       "           -5.4131e-01,  7.7310e-02],\n",
       "          [ 4.2111e-02,  2.1956e-01,  1.8027e-01,  ...,  1.5999e-01,\n",
       "           -2.7667e-01, -4.8190e-01]]], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 3,\n",
       " tensor([[[ 0.3626,  3.1374,  0.1188,  ...,  0.3889, -0.5366, -0.5003],\n",
       "          [ 0.8086, -0.1976,  0.9405,  ...,  0.4102, -0.9600, -0.1172],\n",
       "          [ 0.2109,  0.6325,  0.1781,  ...,  0.3479,  0.0117, -0.6939],\n",
       "          ...,\n",
       "          [-0.1671, -0.0335,  0.0127,  ..., -0.3279, -0.4785,  0.3148],\n",
       "          [-0.0277,  0.4126,  0.7937,  ...,  0.3052, -0.7773, -0.3950],\n",
       "          [-0.1882, -0.0858,  0.0688,  ...,  0.1357, -0.3546, -0.3464]],\n",
       " \n",
       "         [[ 0.2236,  2.5691,  0.1614,  ...,  0.2699, -0.1445, -0.4889],\n",
       "          [ 0.5899, -0.2610,  0.6445,  ...,  0.4102, -0.6636,  0.0595],\n",
       "          [ 0.0176,  0.7142, -0.1884,  ...,  0.1392, -0.3521, -0.4735],\n",
       "          ...,\n",
       "          [ 0.0595, -0.4289,  0.4180,  ...,  0.5223, -0.3705, -0.0305],\n",
       "          [ 0.0873,  0.4177,  0.2576,  ..., -0.1681, -0.5605, -0.0692],\n",
       "          [ 0.3567, -0.0416,  0.2188,  ...,  0.3887, -0.2883,  0.6716]],\n",
       " \n",
       "         [[-0.1173,  2.8430,  0.3294,  ...,  0.4937, -0.2525, -0.7393],\n",
       "          [ 0.6782, -0.1963,  0.6713,  ...,  0.3009, -0.9049, -0.0292],\n",
       "          [ 0.0927,  0.4496,  0.2441,  ...,  0.4149, -0.4392, -0.2746],\n",
       "          ...,\n",
       "          [ 0.3215, -0.4611,  0.5933,  ...,  0.5846, -1.1715,  0.3319],\n",
       "          [-0.0686,  0.1344,  0.0685,  ...,  0.3250, -1.2851,  0.1100],\n",
       "          [-0.1954, -0.5596,  0.2198,  ...,  0.2355, -0.7062, -0.1241]],\n",
       " \n",
       "         [[-0.0857,  2.8221,  0.4374,  ...,  0.4074, -0.3687, -0.5592],\n",
       "          [ 0.5171, -0.2869,  1.2797,  ...,  0.2515, -0.9451, -0.2603],\n",
       "          [-0.0203,  0.8038,  0.6039,  ...,  0.4377, -0.2787, -0.5035],\n",
       "          ...,\n",
       "          [-0.0104, -0.1738,  0.1793,  ...,  0.3595,  0.2346,  0.3806],\n",
       "          [-0.0171, -0.2865,  0.3306,  ...,  0.1102, -0.8887, -0.2420],\n",
       "          [-0.2938,  0.4896, -0.0104,  ...,  0.3317, -0.2939,  0.3248]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model(*xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_ignore_pad(input:Tensor, targ:Tensor, pad_idx)->Rank0Tensor:\n",
    "    n = targ.shape[0]\n",
    "    input = input.argmax(dim=-1).view(n,-1)\n",
    "    targ = targ.view(n,-1)\n",
    "    mask = targ != pad_idx\n",
    "    return (input[mask]==targ[mask]).float().mean()\n",
    "\n",
    "def mask_acc(input:Tensor, t1:Tensor, t2:Tensor)->Rank0Tensor:\n",
    "    return acc_ignore_pad(input[0], t1, vocab.pad_idx)\n",
    "\n",
    "def s2s_acc(input:Tensor, t1:Tensor, t2:Tensor)->Rank0Tensor:\n",
    "    x_mask, task_type, x_task = input\n",
    "    if task_type != TaskType.Translate.value: return torch.tensor(0)\n",
    "    return acc_ignore_pad(x_task, t2, vocab.pad_idx)\n",
    "\n",
    "def ns_acc(input:Tensor, t1:Tensor, t2:Tensor)->Rank0Tensor:\n",
    "    x_mask, task_type, x_task = input\n",
    "    if task_type != TaskType.NextSent.value: return torch.tensor(0)\n",
    "    return accuracy(input[-1], t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics = [mask_acc, ns_acc, s2s_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.345111, tensor(0.1092), tensor(0), tensor(0.)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur_idx = 0\n",
    "# a,b = None, None\n",
    "# for i,(xb,yb) in progress_bar(enumerate(iter(data.train_dl)), total=len(data.train_dl)):\n",
    "#     cur_idx = i\n",
    "#     a,b = xb,yb\n",
    "#     if i == 273: break\n",
    "#     learn.model(*xb)\n",
    "# cur_idx\n",
    "# a[0].min(), a[0].max(), a[2].min(), a[2].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='3', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      33.33% [1/3 13:12<26:24]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mask_acc</th>\n",
       "      <th>ns_acc</th>\n",
       "      <th>s2s_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.069861</td>\n",
       "      <td>3.965626</td>\n",
       "      <td>0.732371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183558</td>\n",
       "      <td>13:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2112' class='' max='4340', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      48.66% [2112/4340 06:26<06:48 3.7822]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(3, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
