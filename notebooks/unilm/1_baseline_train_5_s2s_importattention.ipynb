{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.text import *\n",
    "from enum import Enum\n",
    "import torch\n",
    "from fastai.text.models.awd_lstm import *\n",
    "from fastai.text.models.transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=10, threshold=40, linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "from src.fastai_data import *\n",
    "from src.encode_data import *\n",
    "from src.serve import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.music_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../../data/midi/v15/piano_duet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.models.transformer import _line_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Stream Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctx_len': 150,\n",
       " 'n_layers': 16,\n",
       " 'n_heads': 8,\n",
       " 'd_model': 256,\n",
       " 'd_head': 32,\n",
       " 'd_inner': 2048,\n",
       " 'resid_p': 0.1,\n",
       " 'attn_p': 0.1,\n",
       " 'ff_p': 0.1,\n",
       " 'embed_p': 0.1,\n",
       " 'output_p': 0.1,\n",
       " 'bias': False,\n",
       " 'scale': True,\n",
       " 'act': <Activation.GeLU: 3>,\n",
       " 'double_drop': True,\n",
       " 'tie_weights': True,\n",
       " 'out_bias': True,\n",
       " 'init': <function fastai.text.models.transformer.init_transformer(m)>,\n",
       " 'mem_len': 512,\n",
       " 'mask': True,\n",
       " 'pad_idx': 1,\n",
       " 'bos_idx': 0,\n",
       " 'sep_idx': 8,\n",
       " 'transpose_range': (0, 12),\n",
       " 'note_range': (9, 138),\n",
       " 'bs': 16,\n",
       " 'bptt': 256,\n",
       " 'vocab_size': 274}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = v15s_config(vocab); config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastai Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_tfms = [mask_tfm, next_sentence_tfm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_music_data(path, cache_name='tmp/sample', vocab=vocab, y_offset=0, dl_tfms=dl_tfms, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = data.one_batch(cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[155,  73, 155,  ...,   8, 147,  51],\n",
       "        [155,  60, 155,  ...,   8, 147,   4],\n",
       "        [155,  68, 155,  ...,   8, 147,   4],\n",
       "        ...,\n",
       "        [155,  68, 155,  ...,   8, 147,  52],\n",
       "        [155,  68, 155,  ...,   8, 147,  54],\n",
       "        [155,  73,   4,  ...,   8,   4,  51]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,  68,   1,  ...,   1,   1,  52],\n",
       "         [  1,   1,   1,  ...,   1,   1,  52],\n",
       "         ...,\n",
       "         [155,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,   1, 155,  ...,   1,   1,   1],\n",
       "         [  1,   1, 155,  ...,   1, 147,   1]], device='cuda:0'),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_len = 0\n",
    "# x_len = 16 # bptt\n",
    "# seq_len = m_len+x_len\n",
    "# torch.triu(torch.ones(x_len, seq_len), diagonal=m_len).byte()[None,None].cpu().numpy()\n",
    "# torch.triu(torch.ones(x_len, seq_len), diagonal=m_len+1).byte()[None,None].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    \"Embedding + positional encoding + dropout\"\n",
    "    def __init__(self, vocab_sz:int, emb_sz:int, inp_p:float=0.):\n",
    "        super().__init__()\n",
    "        self.emb_sz = emb_sz\n",
    "        self.embed = embedding(vocab_sz, emb_sz)\n",
    "        self.pos_enc = PositionalEncoding(emb_sz)\n",
    "        self.drop = nn.Dropout(inp_p)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        pos = torch.arange(0, inp.size(1), device=inp.device).float()\n",
    "        return self.drop(self.embed(inp)), self.pos_enc(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTransformer(nn.Module):\n",
    "    \"TransformerXL model: https://arxiv.org/abs/1901.02860.\"\n",
    "    def __init__(self, embed:nn.Module, n_layers:int, n_heads:int, d_model:int, d_head:int, d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., bias:bool=False, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadRelativeAttention,\n",
    "                 learned_pos_enc:bool=False, mask:bool=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = embed\n",
    "        self.u = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.v = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.n_layers,self.d_model,self.mask = n_layers,d_model,mask\n",
    "        self.layers = nn.ModuleList([DecoderLayer(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                      ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "                      attn_cls=attn_cls) for k in range(n_layers)])\n",
    "        \n",
    "        nn.init.normal_(self.u, 0., 0.02)\n",
    "        nn.init.normal_(self.v, 0., 0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs,x_len = x.size()\n",
    "        inp, pos_enc = self.encoder(x)\n",
    "\n",
    "        mask = window_mask(x_len, x.device) if self.mask else None\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            inp = layer(inp, r=pos_enc, u=self.u, v=self.v, mask=mask, mem=None)\n",
    "        core_out = inp[:,-x_len:]\n",
    "        return core_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.learner import _model_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['mem_len'] = 0\n",
    "config['mask'] = False\n",
    "config['bs'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_meta[MusicTransformer] = _model_meta[TransformerXL]\n",
    "_model_meta[MusicTransformer]['config_lm'] = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctx_len': 150,\n",
       " 'n_layers': 16,\n",
       " 'n_heads': 8,\n",
       " 'd_model': 256,\n",
       " 'd_head': 32,\n",
       " 'd_inner': 2048,\n",
       " 'resid_p': 0.1,\n",
       " 'attn_p': 0.1,\n",
       " 'ff_p': 0.1,\n",
       " 'embed_p': 0.1,\n",
       " 'output_p': 0.1,\n",
       " 'bias': False,\n",
       " 'scale': True,\n",
       " 'act': <Activation.GeLU: 3>,\n",
       " 'double_drop': True,\n",
       " 'tie_weights': True,\n",
       " 'out_bias': True,\n",
       " 'init': <function fastai.text.models.transformer.init_transformer(m)>,\n",
       " 'mem_len': 0,\n",
       " 'mask': False,\n",
       " 'pad_idx': 1,\n",
       " 'bos_idx': 0,\n",
       " 'sep_idx': 8,\n",
       " 'transpose_range': (0, 12),\n",
       " 'note_range': (9, 138),\n",
       " 'bs': 4,\n",
       " 'bptt': 256,\n",
       " 'vocab_size': 274}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertHead(nn.Module):\n",
    "    def __init__(self, encoder, mask_decoder, ns_decoder, s2s_decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.mask_decoder = mask_decoder\n",
    "        self.ns_decoder = ns_decoder\n",
    "        self.s2s_decoder = s2s_decoder\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "#         x_emb = self.embed(x)\n",
    "        x_enc = self.encoder(x)\n",
    "        \n",
    "        if y is None: # mask, and next sentence task\n",
    "            return self.mask_decoder(x_enc), self.ns_decoder(x_enc)\n",
    "        \n",
    "        return self.mask_decoder(x_enc), self.s2s_decoder(x_enc, y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.encoder, self.mask_decoder, self.ns_decoder, self.s2s_decoder][idx]\n",
    "        \n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_mask(x_len, device, m_len=0, size=(1,1)):\n",
    "    win_size,k = size\n",
    "    mem_mask = np.zeros((x_len,m_len))\n",
    "    tri_mask = np.triu(np.ones((x_len//win_size+1,x_len//win_size+1)),k=k)\n",
    "    window_mask = tri_mask.repeat(win_size,axis=0).repeat(win_size,axis=1)[:x_len,:x_len]\n",
    "    np_mask = np.concatenate((mem_mask, window_mask), axis=1)\n",
    "    mask = torch.tensor(np_mask, device=device).byte()[None,None]\n",
    "    return mask\n",
    "    \n",
    "def rand_window_mask(x_len,m_len,device,max_size=3,p=0.2,is_eval=False):\n",
    "    if is_eval or m_len == 0 or np.random.rand() >= p: \n",
    "        win_size,k = (1,1)\n",
    "    else: win_size,k = (np.random.randint(0,max_size)+1,0)\n",
    "    return window_mask(x_len, device, m_len, size=(win_size,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_mask(inp, pad_idx:int=1):\n",
    "    return torch.triu(inp.new_ones(inp.size(1),inp.size(1)), diagonal=1)[None,None].byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KVMultiHeadRelativeAttention(nn.Module):\n",
    "    \"MutiHeadAttention with relative positional encoding.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., attn_p:float=0., bias:bool=True,\n",
    "                 scale:bool=True):\n",
    "        super().__init__()\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
    "        \n",
    "        self.q_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.k_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.v_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        \n",
    "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
    "        self.drop_att,self.drop_res = nn.Dropout(attn_p),nn.Dropout(resid_p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.r_attn = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        \n",
    "    def forward(self, q:Tensor, k:Tensor, v:Tensor, \n",
    "                r:Tensor=None, g_u:Tensor=None, g_v:Tensor=None, \n",
    "                mask:Tensor=None, **kwargs):\n",
    "        return self.ln(q + self.drop_res(self.out(self._apply_attention(q, k, v, r, g_u, g_v, mask=mask, **kwargs))))\n",
    "    \n",
    "    def _apply_attention(self, q:Tensor, k:Tensor, v:Tensor, \n",
    "                         r:Tensor=None, g_u:Tensor=None, g_v:Tensor=None, \n",
    "                         mask:Tensor=None):\n",
    "        #Notations from the paper: x input, r vector of relative distance between two elements, u et v learnable\n",
    "        #parameters of the model common between all layers, mask to avoid cheating and mem the previous hidden states.\n",
    "        bs,x_len,seq_len = q.size(0),q.size(1),r.size(0)\n",
    "        wq,wk,wv = self.q_wgt(q),self.k_wgt(k),self.v_wgt(v)\n",
    "        wq = wq[:,-x_len:]\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        wkr = self.r_attn(r)\n",
    "        wkr = wkr.view(seq_len, self.n_heads, self.d_head)\n",
    "        wkr = wkr.permute(1,2,0)\n",
    "        #### compute attention score (AC is (a) + (c) and BS is (b) + (d) in the paper)\n",
    "        AC = torch.matmul(wq+g_u,wk)\n",
    "        BD = _line_shift(torch.matmul(wq+g_v, wkr))\n",
    "        if self.scale: attn_score = (AC + BD).mul_(1/(self.d_head ** 0.5))\n",
    "        if mask is not None: \n",
    "            attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return attn_vec.permute(0, 2, 1, 3).contiguous().view(bs, x_len, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2SDecoderBlock(nn.Module):\n",
    "    \"Decoder block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0., ff_p:float=0.,\n",
    "                 bias:bool=True, scale:bool=True, double_drop:bool=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha1 = KVMultiHeadRelativeAttention(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.mha2 = KVMultiHeadRelativeAttention(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.ff   = feed_forward(d_model, d_inner, ff_p=ff_p, double_drop=double_drop)\n",
    "    \n",
    "    def forward(self, targ:Tensor, enc:Tensor, \n",
    "                r=None, u=None, v=None,\n",
    "                mask_in:Tensor=None, mask_out:Tensor=None): \n",
    "        y = self.mha1(targ, targ, targ, r, u, v, mask=mask_out)\n",
    "        return self.ff(self.mha2(y, enc, enc, r, u, v, mask=mask_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2SDecoder(nn.Module):\n",
    "    def __init__(self, embed:nn.Module, n_hid:int, vocab_sz:int, n_layers:int, n_heads:int, d_model:int, d_head:int, d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., bias:bool=False, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadRelativeAttention,\n",
    "                 learned_pos_enc:bool=False, mask:bool=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = embed\n",
    "        self.u = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.v = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.n_layers,self.d_model,self.mask = n_layers,d_model,mask\n",
    "        self.layers = nn.ModuleList([S2SDecoderBlock(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                      ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "                      attn_cls=attn_cls) for k in range(n_layers)])\n",
    "        self.head = MusicLinearDecoder(d_model, vocab_sz, tie_encoder=embed.embed, **kwargs)\n",
    "    \n",
    "        nn.init.normal_(self.u, 0., 0.02)\n",
    "        nn.init.normal_(self.v, 0., 0.02)\n",
    "        \n",
    "    def forward(self, enc, targ):\n",
    "        # x = encoder, y = target\n",
    "        bs,targ_len = targ.size()\n",
    "        \n",
    "        targ_emb, pos_enc = self.encoder(targ)\n",
    "\n",
    "#         mask = window_mask(x_len, x.device) if self.mask else None\n",
    "        mask_out = window_mask(targ_len, targ_emb.device)\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            out = layer(targ_emb, enc, mask_out=mask_out,\n",
    "                        r=pos_enc, u=self.u, v=self.v)\n",
    "        return self.head(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class S2SDecoder(nn.Module):\n",
    "#     def __init__(self, embed, n_hid, vocab_sz, n_layers, **kwargs):\n",
    "#         super().__init__()\n",
    "#         self.decoder = nn.ModuleList([S2SDecoderBlock(**kwargs) for _ in range(n_layers)])\n",
    "#         self.head = MusicLinearDecoder(n_hid, vocab_sz, tie_encoder=embed, **kwargs)\n",
    "        \n",
    "# #         self.pad_idx = pad_idx\n",
    "        \n",
    "#     def forward(self, inp, out):\n",
    "#         x_len = inp.shape[-1]\n",
    "# #         mask_out = rand_window_mask(x_len, 0, inp.device, is_eval=not self.training)\n",
    "#         mask_out = window_mask(x_len, inp.device)\n",
    "    \n",
    "#         out = self.embed(out)\n",
    "#         for dec_block in self.decoder: out = dec_block(out, inp, mask_in, mask_out)\n",
    "#         return self.head(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MusicLinearDecoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_hid:int, n_out:int, output_p:float, tie_encoder:nn.Module=None, bias:bool=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        output = self.output_dp(input)\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_music_model(vocab_sz:int, config:dict=None, drop_mult:float=1.):\n",
    "    \"Create a language model from `arch` and its `config`, maybe `pretrained`.\"\n",
    "    for k in config.keys(): \n",
    "        if k.endswith('_p'): config[k] *= drop_mult\n",
    "#     tie_weights,output_p,out_bias = map(config.pop, ['tie_weights', 'output_p', 'out_bias'])\n",
    "    tie_weights,output_p,out_bias = map(config.get, ['tie_weights', 'output_p', 'out_bias'])\n",
    "    n_hid = config['d_model']\n",
    "    embed = TransformerEmbedding(vocab_sz, n_hid, inp_p=config['embed_p'])\n",
    "    encoder = MusicTransformer(embed=embed, **config)\n",
    "    mask_decoder = MusicLinearDecoder(n_hid, vocab_sz, output_p, tie_encoder=embed.embed, bias=out_bias)\n",
    "    ns_decoder = MusicLinearDecoder(n_hid, 4, output_p, tie_encoder=None, bias=out_bias)\n",
    "    s2s_decoder = S2SDecoder(embed, n_hid, vocab_sz, **config)\n",
    "    model = BertHead(encoder, mask_decoder, ns_decoder, s2s_decoder)\n",
    "    return model.apply(init_transformer)\n",
    "\n",
    "\n",
    "def music_model_learner(data:DataBunch, config:dict=None, drop_mult:float=1., pretrained:bool=False,\n",
    "                        pretrained_fnames:OptStrTuple=None, **learn_kwargs) -> 'LanguageLearner':\n",
    "    \"Create a `Learner` with a language model from `data` and `arch`.\"\n",
    "    model = get_music_model(config['vocab_size'], config=config, drop_mult=drop_mult)\n",
    "    \n",
    "    meta = _model_meta[TransformerXL]\n",
    "    learn = MusicLearner(data, model, split_func=meta['split_lm'], \n",
    "                         bos_idx=config['bos_idx'], sep_idx=config['sep_idx'],\n",
    "                        **learn_kwargs)\n",
    "    \n",
    "    if pretrained:\n",
    "        if 'url' not in meta: \n",
    "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "            return learn\n",
    "        model_path = untar_data(meta['url'], data=False)\n",
    "        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        learn.load_pretrained(*fnames)\n",
    "        learn.freeze()\n",
    "    if pretrained_fnames is not None:\n",
    "        fnames = [learn.path/learn.model_dir/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]\n",
    "        learn.load_pretrained(*fnames)\n",
    "        learn.freeze()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sep_idx: 8\n"
     ]
    }
   ],
   "source": [
    "learn = music_model_learner(data, config.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = data.one_batch(cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLoss():\n",
    "    def __init__(self, mask_loss, sent_loss):\n",
    "        self.mask_loss = mask_loss\n",
    "        self.sent_loss = sent_loss\n",
    "        \n",
    "    def __call__(self, input:Tensor, target:Tensor, target_sen:Tensor, **kwargs)->Rank0Tensor:\n",
    "        m = self.mask_loss.__call__(input[0], target, **kwargs)\n",
    "        s = self.sent_loss.__call__(input[1], target_sen, **kwargs)\n",
    "        return m + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTrainer(LearnerCallback):\n",
    "    \"`Callback` that regroups lr adjustment to seq_len, AR and TAR.\"\n",
    "    def __init__(self, learn:Learner):\n",
    "        super().__init__(learn)\n",
    "        \n",
    "#     def on_loss_begin(self, last_output:Tuple[Tensor,Tensor,Tensor], **kwargs):\n",
    "#         \"Save the extra outputs for later and only returns the true output.\"\n",
    "#         return {'last_output': (last_output[0], last_output[1]) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.callbacks = [BertTrainer(learn, alpha=2, beta=1)]\n",
    "learn.callbacks = [BertTrainer(learn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = BertLoss(CrossEntropyFlat(ignore_index=vocab.pad_idx), CrossEntropyFlat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0743,  0.3196,  0.3367,  ...,  0.2344,  0.2848, -0.0412],\n",
       "          [ 0.3629,  0.2655,  0.1444,  ...,  0.0260,  0.1932, -0.5172],\n",
       "          [-0.0354, -0.1314, -0.3341,  ...,  0.1010,  0.1620, -0.1120],\n",
       "          ...,\n",
       "          [-0.2748,  0.2047,  0.2082,  ...,  0.2025,  0.2456,  0.6037],\n",
       "          [-0.0886,  0.0361,  0.3315,  ...,  0.2643,  0.4652,  0.4201],\n",
       "          [ 0.1788, -0.2791,  0.1327,  ...,  0.0840,  0.3184,  0.0697]],\n",
       " \n",
       "         [[ 0.4849, -0.0831,  0.0960,  ..., -0.1094,  0.6874,  0.0190],\n",
       "          [ 0.1493, -0.2390,  0.0464,  ..., -0.0780,  0.7060, -0.0755],\n",
       "          [ 0.3525, -0.1121,  0.0144,  ...,  0.4640,  0.1898,  0.1246],\n",
       "          ...,\n",
       "          [-0.0620,  0.1672,  0.0721,  ...,  0.2152,  0.3309,  0.4012],\n",
       "          [ 0.6216, -0.0695, -0.1472,  ..., -0.3827,  0.2661, -0.1737],\n",
       "          [ 0.3156,  0.0521,  0.3747,  ..., -0.1284,  0.4987,  0.0275]],\n",
       " \n",
       "         [[ 0.3770,  0.4130, -0.2621,  ..., -0.3210,  0.1762, -0.0058],\n",
       "          [ 0.1668,  0.3123,  0.1441,  ...,  0.1478,  0.1747, -0.0673],\n",
       "          [ 0.3274, -0.1545, -0.2544,  ...,  0.0779,  0.2661,  0.1736],\n",
       "          ...,\n",
       "          [-0.3432,  0.4389, -0.0515,  ...,  0.6257,  0.5912,  0.2218],\n",
       "          [-0.3347,  0.1159, -0.0837,  ...,  0.3060,  0.6809,  0.1624],\n",
       "          [ 0.0563,  0.2495,  0.0572,  ..., -0.0757,  0.3643,  0.2063]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.1940, -0.0177,  0.0254,  ..., -0.4997,  0.0766, -0.4228],\n",
       "          [ 0.2982,  0.0051,  0.1032,  ...,  0.1633,  0.1571,  0.0321],\n",
       "          [ 0.2081, -0.1492,  0.0543,  ..., -0.3681,  0.3207, -0.3122],\n",
       "          ...,\n",
       "          [-0.1039, -0.0043,  0.1541,  ..., -0.1477,  0.2964,  0.5071],\n",
       "          [-0.2485,  0.0070, -0.0954,  ...,  0.1170,  0.3055,  0.3097],\n",
       "          [ 0.0091,  0.0403,  0.1733,  ...,  0.2759, -0.0579, -0.2414]],\n",
       " \n",
       "         [[ 0.4410,  0.2353,  0.8270,  ...,  0.2844, -0.2244, -0.2729],\n",
       "          [ 0.1916,  0.0997,  0.2551,  ..., -0.2041,  0.0288, -0.2982],\n",
       "          [ 0.0875, -0.3627, -0.2110,  ...,  0.3207,  0.1803,  0.0524],\n",
       "          ...,\n",
       "          [ 0.0954,  0.2466,  0.2063,  ...,  0.1146,  0.4537,  0.7119],\n",
       "          [ 0.2230,  0.0605,  0.5065,  ...,  0.2645,  0.1398, -0.3653],\n",
       "          [ 0.1530,  0.2566,  0.1674,  ..., -0.1096,  0.1236, -0.2357]],\n",
       " \n",
       "         [[-0.1336, -0.3978, -0.3559,  ..., -0.1329,  0.1065,  0.3937],\n",
       "          [ 0.0963,  0.0629,  0.1069,  ..., -0.2835,  0.2389, -0.2170],\n",
       "          [ 0.2083, -0.0675, -0.4624,  ...,  0.1562,  0.5758,  0.4763],\n",
       "          ...,\n",
       "          [ 0.2176,  0.2308,  0.0618,  ..., -0.0874,  0.6086,  0.6216],\n",
       "          [-0.1973, -0.0212,  0.0099,  ...,  0.2224,  0.5975,  0.1968],\n",
       "          [ 0.0194,  0.0506,  0.2847,  ...,  0.3881,  0.0405, -0.0533]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-2.1074e-01,  3.9008e+00,  5.5367e-01,  ...,  1.4516e-01,\n",
       "            7.1303e-02, -2.9054e-01],\n",
       "          [-1.0200e-01,  3.9688e+00,  3.8064e-01,  ...,  2.8035e-01,\n",
       "            1.9445e-01, -1.9478e-01],\n",
       "          [-2.1128e-01,  3.8167e+00,  5.2783e-01,  ...,  3.4308e-01,\n",
       "            2.1991e-01, -4.3966e-01],\n",
       "          ...,\n",
       "          [ 4.8950e-02,  3.8812e+00,  2.6225e-01,  ...,  1.9992e-01,\n",
       "            1.2208e-01, -2.7820e-01],\n",
       "          [-4.4109e-02,  3.9359e+00,  4.7560e-01,  ..., -4.2770e-03,\n",
       "            1.1812e-01, -2.1221e-01],\n",
       "          [-1.5992e-01,  3.9215e+00,  5.4603e-01,  ...,  3.1750e-01,\n",
       "            1.1849e-01, -3.1937e-01]],\n",
       " \n",
       "         [[-2.0258e-01,  4.2653e+00,  5.5082e-01,  ...,  6.3051e-01,\n",
       "            4.2707e-01, -1.2119e-01],\n",
       "          [-2.5450e-01,  4.2957e+00,  1.9267e-01,  ...,  3.9802e-01,\n",
       "            1.8620e-01, -2.1306e-01],\n",
       "          [-2.5511e-01,  4.3368e+00,  3.3487e-01,  ...,  2.3469e-01,\n",
       "            2.8318e-01, -2.9918e-01],\n",
       "          ...,\n",
       "          [-8.5857e-02,  4.1284e+00,  3.5704e-01,  ...,  4.6681e-01,\n",
       "            2.2164e-02, -2.1764e-01],\n",
       "          [-2.8385e-01,  4.2933e+00,  2.9864e-01,  ...,  3.4891e-01,\n",
       "            3.6417e-02, -3.0503e-01],\n",
       "          [-1.4909e-01,  4.2397e+00,  3.9684e-01,  ...,  2.7911e-01,\n",
       "            2.2965e-01, -1.8776e-01]],\n",
       " \n",
       "         [[ 6.8889e-02,  4.3644e+00,  4.9853e-01,  ...,  4.6680e-01,\n",
       "            1.9016e-01, -3.2518e-01],\n",
       "          [ 3.0444e-02,  4.2729e+00,  4.9441e-01,  ...,  2.5502e-01,\n",
       "            5.3206e-02, -2.4528e-01],\n",
       "          [-1.4096e-01,  4.4726e+00,  3.4495e-01,  ...,  5.2694e-01,\n",
       "            4.2634e-02, -2.6426e-01],\n",
       "          ...,\n",
       "          [-2.3793e-01,  4.4018e+00,  2.7422e-01,  ...,  5.0653e-01,\n",
       "            8.0489e-02, -2.5049e-01],\n",
       "          [-3.2649e-01,  4.1981e+00,  2.0590e-01,  ...,  5.2037e-01,\n",
       "            9.9983e-03, -3.7879e-01],\n",
       "          [-1.1442e-01,  4.4116e+00,  6.8594e-01,  ...,  4.0262e-01,\n",
       "            9.8872e-02, -2.4923e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 3.4151e-01,  4.9766e-01,  4.2405e-01,  ..., -4.4017e-03,\n",
       "            4.2033e-01, -2.0511e-01],\n",
       "          [ 1.2748e-01,  4.1180e+00,  2.7860e-01,  ...,  2.8530e-01,\n",
       "            1.2812e-01, -3.3219e-01],\n",
       "          [ 6.5795e-03, -1.7355e-01, -3.0804e-01,  ...,  5.0425e-01,\n",
       "           -1.5208e-01, -2.2097e-02],\n",
       "          ...,\n",
       "          [-8.2481e-02,  4.1642e+00,  3.3381e-01,  ...,  4.1965e-01,\n",
       "           -1.1865e-02, -3.3616e-01],\n",
       "          [-4.0433e-03,  4.0511e+00,  4.1356e-01,  ...,  3.7721e-01,\n",
       "            6.1234e-02, -5.5975e-02],\n",
       "          [-1.0480e-01,  4.1660e+00,  4.1062e-01,  ...,  5.0711e-01,\n",
       "            5.7212e-02, -3.0959e-01]],\n",
       " \n",
       "         [[ 2.2496e-01,  4.0671e+00,  5.6308e-01,  ...,  2.0978e-01,\n",
       "            9.7756e-02, -7.3877e-02],\n",
       "          [-8.4052e-02,  3.9255e+00,  3.6271e-01,  ...,  5.7776e-01,\n",
       "            1.2732e-01, -1.7346e-01],\n",
       "          [ 9.6302e-02,  4.2237e+00,  4.5751e-01,  ...,  2.2180e-01,\n",
       "           -2.1772e-02, -2.2271e-01],\n",
       "          ...,\n",
       "          [-3.6843e-02,  3.9989e+00,  4.5986e-01,  ...,  5.0907e-01,\n",
       "            2.6158e-01, -1.8797e-01],\n",
       "          [ 1.1548e-01,  4.1196e+00,  5.7062e-01,  ...,  1.8969e-01,\n",
       "            6.0964e-02, -2.3494e-01],\n",
       "          [-4.1730e-02,  4.1091e+00,  3.9782e-01,  ...,  4.0247e-01,\n",
       "            1.8864e-01, -3.8356e-01]],\n",
       " \n",
       "         [[ 3.6055e-02,  4.6049e+00,  5.6244e-01,  ...,  6.3386e-01,\n",
       "            2.2268e-02, -3.0074e-01],\n",
       "          [-1.4402e-01,  4.4886e+00,  3.5906e-01,  ...,  3.2579e-01,\n",
       "            1.3041e-01, -3.9031e-01],\n",
       "          [ 1.4571e-03,  4.5389e+00,  3.4926e-01,  ...,  4.1709e-01,\n",
       "            1.3065e-03, -4.3397e-01],\n",
       "          ...,\n",
       "          [-1.5072e-01,  4.5543e+00,  2.3560e-01,  ...,  6.9177e-01,\n",
       "            1.1760e-01, -4.2377e-01],\n",
       "          [ 3.4123e-02,  4.5221e+00,  2.6763e-01,  ...,  4.7286e-01,\n",
       "            2.2874e-01, -4.5043e-01],\n",
       "          [ 1.6749e-01,  4.6047e+00,  3.0807e-01,  ...,  5.8584e-01,\n",
       "            2.5339e-01, -4.7176e-01]]], device='cuda:0', grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model(xb, yb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_acc(input:Tensor, t1:Tensor, t2:Tensor)->Rank0Tensor:\n",
    "    n = t1.shape[0]\n",
    "    input = input[0].argmax(dim=-1).view(n,-1)\n",
    "    t1 = t1.view(n,-1)\n",
    "    mask = t1 != vocab.pad_idx\n",
    "    return (input[mask]==t1[mask]).float().mean()\n",
    "\n",
    "def ns_acc(input:Tensor, t1:Tensor, t2:Tensor)->Rank0Tensor:\n",
    "    return accuracy(input[1], t2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics = [mask_acc, ns_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.7785196, tensor(0.1001), tensor(0.3444)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mask_acc</th>\n",
       "      <th>ns_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.288463</td>\n",
       "      <td>3.077294</td>\n",
       "      <td>0.539648</td>\n",
       "      <td>0.576416</td>\n",
       "      <td>02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.314284</td>\n",
       "      <td>2.585530</td>\n",
       "      <td>0.588401</td>\n",
       "      <td>0.324031</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.121686</td>\n",
       "      <td>2.169067</td>\n",
       "      <td>0.596297</td>\n",
       "      <td>0.499136</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(3, 1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
